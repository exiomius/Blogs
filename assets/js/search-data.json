{
  
    
        "post0": {
            "title": "Fast.ai Lessons 8 Roundup",
            "content": "Lesson Overview . (This post is in progress.) . The topics covered, briefly . Lecture Notes . Back to collaborative filtering notebook, about embedding matrix. | PyTorch keeps track of neural network parameters/weights for you. | movie_bias is the interference parameter we created in order to add bias for movies, user_bias is the one we created in order to add bias for users. | How does our model work? To predict, it trains as normal on the input ratings and output movie ratings, but then we add meaningful interference parameters, movie_bias and user_bias for each movie and user respectively, to adjust with domain specific knowledge. Some movies just everyone likes, so are biased, some users just like every movie, so are biased. | Visualsing embeddings: Plotting a principle component analysis of PCA component 1 and 2, gives a compressed view of how our latent factors effect eachother. It gives us a graph to interpret how the top latent factors are related to eachother. Domain specific knowledge would tell us what there PCA components/inference factors are, and we can try and understand why they relate the way they do. | Embedding distance: calculate how far apart each embedding vector is from a specific movie, aka how similar each movie is compared to that movie based on the latent factors. | Using deep learning for collaborative filtering instead: We use a neural net to try and enter the missing values in the matrix. | we use fast.ai to try and find the embedding matrix side/number of latent variables. | In practice, a combined approach of a neural net and dot product approach is best for collaborative filtering. | In collab filtering, often a small number of users and movies overwealm the rest. In anime for example, some users watch so much anime compared to the rest and this screws up predictions for normal users. It&#39;s a higher level task to try and resolve this. | Embedding in NLP: an embedding matrix is just a matrix of latent variables for every word. And as before, the embedding distance calculates the embedding distance between a word and other words, telling us how similar they are based on latent variables. | For tabular data, using a tabular model/learner for it, creates an embedding matrix, and uses it. | Collab filtering, NLP, tabular data neural nets, all use embedding matrices. | Using embedding, latent variables, may be a substitute to doing a lot of feature engineering. | Create and train a neural net, take its embeddings and use it with other models like random forests, it can give a nice performance boost. | One latent variable found to help predict retail store sales, was distance in real life. Actually the embedding matrix&#39;s distances for that latent variable matched the distances in real life! It learnt IRL distance was important for predicting retail sales and reflected that! | Convolutions: A CNN is similar to the neural networks before, but for computer vision they have a particular feature: it can find features of images like horizontal and vertical lines of handwritten numbers. | How? It has a filter matrix, say a 3x3 one, and moves around, dot producting and RELUing each 3x3 subset matrix of the original image, producing a smaller image. This is called a convolution. | Why does this work? Because of the values in the filter. E.g. we used a 3x3 matrix with the top elements = 1, middle elements = 0, bottom = -1. The dot product of this will only give the highest value when there&#39;s horizontal image in the top row, anything in the middle, and no image in the bottom, aka whenever there&#39;s a horizontal part of an image! In vertical parts, the bottom row will cancel out the top and produce 0s, aka the convolution layer will have nothing. | The 3x3 matrix is called a filter. | We can have a horizontal edges conv layer, and a vertical edges conv layer. | We use another filter, but with two matrices, one for each conv layer, and to combine their features. | The way we find the kernal values is to set them as parameters and use SGD to optimise. It will find the best kernal parameters for optimum digit classification. | Nowadays we do a stride convoultion, e.g. stride 2, by skipping values. This reduces the grid size by 2x2. The grid size is not reduced by the kernal size, it is reduced determined by the stride we set. | When we&#39;re done with stride convos, with aboug 7x7 image at the end, we do an average pool, we average the final activations. | Say we&#39;re trying to identify a picture of a bear. If the bear is a small part of the picture, max pool is much better than avg pool at this. It checks each end activation for a bear, rather than having one prediction of bear or not bear for all the activations combined. | Depending on how you want your model to work, pick max or avg pool accordingly. Fast.ai does this for you, it actually does an average of max/avg pool that tries both for you. | A convolution, all the kernal multiplies done, mathematically is equivilent to just one big matrix multiplcation. | Dropout: we can have a dropout mask, we multiply it by our filtered image, to delete random bits of it. Higher dropout means the image is harder to see. A human is able to look at a droppedoutted image and still know what the image is. A model should be able to as well, if we dropout, perhaps it is forced to learn more fundamental features, will should help against overfitting a lot. | There are more activation functions/activations than ReLU. This doesn&#39;t matter much so we don&#39;t care. | What to do before part 2: | Read Meta Learning: How to Learn Deep Learning. | Watch the videos again and code and experiment as you go. | Spend time on the forums. | Get together with others to study together. | Build projects. | . Questions . Links . EX . The course page for this sessions is https://course.fast.ai/Lessons/lesson8.html, which includes a lecture, notebooks, and a set of questions from the course book. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson8Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/27/Lesson8Blog.md.html",
            "date": " • Sep 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fast.ai Lessons 7 Roundup",
            "content": "Lesson Overview . (This post is in progress.) . The topics covered, briefly . Lecture Notes . Whats inside a neural net. Tweaks to the basic neural net we constructed previously. Going through road to the top, part 3. . A larger model has more parameters so can find more features, but the problem is that it takes GPU memory that isn&#39;t as flexible as CPU memory. | How to use as large a model as you like. Kaggle is 16Gb GPU memory. |You can find how much memory a model uses. Training for longer does not use more memory. | Gradient accumeleration to avoid memory overuse problem. We run smaller batch sizes, but modify them as to act and train as if we were using the same normal batch size for all the training data. | Gradient accumleration results are identicle to using a higher memory GPU for certain models. It is for convText and Transformers (NLP). If a model uses batch normalisation, then it won&#39;t exactly, but will still be a good model. | Pick a batch size that fits your GPU memory, and generally higher and a multiple of 8 is better. Generally (not always) if you double batch size, half learning rate. | We use ensembling bagging on good models of different arctextures. | At the start it may feel random as to why certain approaches/models are better, but over time as you develop intuiton, it will be less random and more systematic. | Generally, it makes sense to iterate on small models then switch to large models, but there&#39;s a better way of ensuring this performance converts correctly. Covered in the second course. Now using part 4 rttp: . | Predicting two things instead of one, two dependant variables instead of one. E.g. from a rice photo, the type of rice (10 types) and the disease it may have (10 types). . | This requires an understanding of making custom loss functions and a deeper look into how cross entropy loss works. | Make a learner just for disease, and create a new metric function for it. | Cross entropy loss: It goes into the maths using a seperate excel sheet. E.g, trying to predict if a image is a cat,dog,plane,fish or building. Your model outputs 5 numbers (relating to probabilities of each category). CEL first finds the softmax value for each of them. Then it compares the actual value (1 for the correct category, 0 for not) to the softmax value for each category. It multiplies the log of the probability prediction for the correct category by the actual value. | Further info: https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/ | Binary cross entropy is just cross entropy for 1 category: is a cat or not. | The loss functions have two types. The F function type and the nn class type. The latter has more parameters to play with. | Change the last node outputs to be the number of categories predicted instead of the usual 1 for classification. | You encode the loss function for the model to know what/how many categories to predict. You sum loss functions for multiple category types and their sub categories. | This new model, that can predict 20 categories, actually is better than a model that just predicts disease type! This is because the training to do other types of predictions helps. Sometimes this approach is better, sometimes not! | . Notebook collab filtering deep dive . A key part of recommender systems | Movielens dataset of movie ratings. UserID, MovieID, and rating. | How to predict how a user will rate a unrated movie for them: match up the user&#39;s movie preferences with the movie&#39;s features to give a number prediction. | But we don&#39;t know their preferences and the movie features, these are called latent variables, and we can infer them from the dataset. | Let&#39;s assume there are 5 latent factors, say like for a movie, it&#39;s genre, length etc, we don&#39;t set these, we infer them and then can try and interpret what they are. | On choosing the number of latent factors, its hard. Fast.ai has a function to calculate this based on Jeremy&#39;s intuition, but you can play around too. | Use SGD to optimise these latent factors after we set a loss function. | Imagine a matrix of the users and their movie ratings. There are missing values for unrated/unseen movies. Collaborative filtering is matrix completion, trying to fill in these missing values. | What is embedding? Just looking something up in an array. An embedding matrix is the array that is looked up. Matrix multiplication in an embedding matrix is the same as looking up index values in a list say as a function in excel. Think about a dot product with a one-hot encoded vector, it just returns the value you&#39;re looking up. | Creating a collaborative filtering model from scratch. | Covering python, PyTorch class definition and features. | We create a DotProduct class to define embeddings and looking up values for UserIDs and MovieIDs. | Some of our predictions can greater than 5, the maximum. Take our predictions and squish them with a sigmoid to fix this. | Some users just relates all movies highly, some users have a range of ratings. Let&#39;s incoperate this. Make another inference variable, a movie bias and a user bias, reflecting that for movies, they tend to be especially related well or badly, and that for some users, they can rate all movies generally as good or bad. | With more column features, we could cluster users instead to try and incoperate user and movie types/preferences? | We can use L2 regularisation (weight decay), to avoid overfitting. This adds the sum of the square of the weights to the loss function. This also solves the issue of having useless interfered variables, because they won&#39;t contribute. | In fast.ai, usually defaults are good, but for tabular data, it&#39;s hard to know good defaults, so it&#39;s good to test yourself. | . Questions . The questions from now on will be a part of the blog post rather than uploaded to the lesson&#39;s GitHub repo. . What problem does collaborative filtering solve? | How does it solve it? | Why might a collaborative filtering predictive model fail to be a very useful recommendation system? | What does a crosstab representation of collaborative filtering data look like? | Write the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!). | What is a latent factor? Why is it &quot;latent&quot;? | What is a dot product? Calculate a dot product manually using pure Python with lists. | What does pandas.DataFrame.merge do? | What is an embedding matrix? | What is the relationship between an embedding and a matrix of one-hot-encoded vectors? | Why do we need Embedding if we could use one-hot-encoded vectors for the same thing? | What does an embedding contain before we start training (assuming we&#39;re not using a pretained model)? Create a class (without peeking, if possible!) and use it. | What does x[:,0] return? | Rewrite the DotProduct class (without peeking, if possible!) and train a model with it. | What is a good loss function to use for MovieLens? Why? | What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model? | What is the use of bias in a dot product model? | What is another name for weight decay? | Write the equation for weight decay (without peeking!). | Write the equation for the gradient of weight decay. Why does it help reduce weights? | Why does reducing weights lead to better generalization? | What does argsort do in PyTorch? Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not? | How do you print the names and details of the layers in a model? | What is the &quot;bootstrapping problem&quot; in collaborative filtering? | How could you deal with the bootstrapping problem for new users? For new movies? | How can feedback loops impact collaborative filtering systems? | When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users? | Why is there an nn.Sequential in the CollabNN model? | What kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model? | . Links . EX . The course page for this sessions is https://course.fast.ai/Lessons/lesson7.html, which includes a lecture, notebooks, and a set of questions from the course book. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson7Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/27/Lesson7Blog.md.html",
            "date": " • Sep 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fast.ai Lessons 6 Roundup",
            "content": "Lesson Overview . This lesson focused on the main alternate to deep learning: Random Forests and Gradient Boosting. There is an excellent summary at the end of the book&#39;s chapter giving context into why and when to use deep learning and/or RF/GB. Essentially, the latter is great for tabular data, and the former for more complex messy input like natural language, images etc. How to create a Random Forest from scratch is covered, as well as some links describing Gradient Boosting in more depth. Lastly, the notebooks road to the top part 1 and part 2 are covered giving more insight into how to create good ML models in a competitive setting. . The topics covered, briefly . Random Forests: how to create them from scratch, how they are hard to do incorrectly, how they are hard to overfit, how their hyperparamaters work, what bagging and out-of-bag error is, how they are great for tabular data and are especially good due to their easy understandability and interpretation. | OneR vs TwoR classifier differences and creation. | By comparing OOB error with validation error, you can tell if there are issues with the way your validation data is constructed and/or if there is data leakage. | A list of vision models to use sorted by iteration speed and accuracy. | More insight how to compete in machine learning competitions by creating good validation sets and iterating quickly. | Issues with model iteration speed and CPU/GPU usage. Ideally, pick a model that can utilise both correctly instead of being bottlenecked by either. | . Lecture Notes . Binary splits. They find and use the best value to split a dataset into two parts to predict the dependent variable. | We found Sex to be the best predictor from the columns for surviving the titanic, with the score to be about 0.4, a score being a measure of how good the split is at predicting. | If we predicted all women to survive and all men to die, it would do a decent job. This is a called a OneR classifier. | How to improve this? Use a TwoR classifier, look into the male and female groups separately and use another column to split again! | This gives us a decision tree! A series of binary splits. DecisionTreeClassifier, from sklearn, does it for us. It takes leafnodesize as a variable, meaning for the final groups at the end, at minimum how many datapoints need to be in them. | Gini is like score, it&#39;s another metric for measuring how good the split is. | TwoR might not necessarily be better than OneR at predicting, especially for small datasets. | For tabular data, always use a decision tree. It requires very little preprocessing (don&#39;t have to convert categorical data, for example) and is a good baseline. | Bagging: Build many decision trees using different binary splits and subsections of the training data. Each tree will have an unbiased amount of error: error that is random and uncorrelated. If we average out the errors that are random and uncorrelated we can get very little error left! So build an ensemble of decision trees! This is called a random forest. | For datasets of decent size, Random Forests almost always are better than OneR. | A feature importance plot tells us how important each feature is! It&#39;s amazing for understandability. Maybe for a tabular dataset, we use a random forest feature importance plot first for a baseline and understanding of feature importance, then create a DL model with that comparison and understanding in mind. | After about 30 trees, the improvement for random trees doesn&#39;t improve massively. Roughly &lt; 100 works best. More forests does not cause overfitting, in fact, having too few trees might. This is the same as having a small ensemble of overfitted models could be overfitted, but a large ensemble will not be overfitted. | Because when creating trees, we didn&#39;t train each one on all of the training data, say on a random subsection of 0.75 of it. We can then use the final 0.25 section as validation data for each tree. Then average this error across all our trees. This is called the OOB (out-of-bag) error. | For the random forest, we might be able to get away without using validation data to see if there&#39;s overfitting by using OOB instead. | Random forests are amazing for model interpretation. It gives confidence in predictions, which columns are strong predictors, how related are columns are to eachother etc. | If all the trees are predicting very different things for the same entry, then we are not very confident. | A partial dependance plot uses the forest&#39;s predictions to set all the other data equal and tells us how two variables are related. There is an important distinction. Say we want to know how car price depends on the year of manufacture. We could just plot a graph of car prices and manufacture year. This however would depend on other variables: say another present column, air conditioning (yes/no), greatly improves the price. In contrast, PDP will get the prediction input data and make all the columns&#39; data the same except for year of manufacture. Then it will make predictions for car price using that, and plot it against year of manufacture. Unlike the first plot, it tells us a different measure of how year of manufacture affects price. | Gradient boosting is another meta technique using binary splits like random forests. It&#39;s can more accurate but can overfit so can be done wrong. More info here: https://explained.ai/gradient-boosting/. | . The lecture than goes through a more in depth notebooks than Iterate like a grandmaster called road to the top part 1 and part 2: -Just squish images, it&#39;s better than cropping. Padding would take more precious CPU power for iteration? (See point below). TTA, augmenting input images, can improve accuracy too. . There is another notebook: https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning, showing the best models for vision to use for quick iteration and testing. | The fastai learning rate suggestions are a bit conservative so you can pick higher than them. | Create different notebooks for different approaches. Duplicate and rename them. | Kaggle&#39;s GPUs aren&#39;t amazing, but for Jeremy his home PC ran training so much faster. The problem wasn&#39;t GPU, it was CPU, the Kaggle CPU indicator showed it as full. This is a image input problem. Loading them up requires CPU power. Simply resizing the images by 1/4 quickened up iteration by 4x but without sacrificing accuracy! Perhaps larger image sizes aren&#39;t so important. But later on, with some data augmentation, using larger image sizes helped. | Since the GPU was barely used, you might as well switch to a model that&#39;s more demanding. It probably won&#39;t take much longer. | Keep upto date with new model architecture. ConvNext is a great default speed and performance for now. | . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson6.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/26/Lesson6Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/26/Lesson6Blog.md.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fast.ai Lessons 5 Roundup",
            "content": "Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson5.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/26/Lesson5Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/26/Lesson5Blog.md.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Masters Project 04/10/2022",
            "content": "Meeting 04/10/2022. . During the meeting we discussed: . What I had been learning during the summer, namely the first fast.ai course, the state of exponential growth of machine learning, my website, etc. We are all happy with the progress I have made so far. I will estimate that I spent around 40 hours working this summer, ahead of the 30 hours the department set. | For example, I showed how I can upload machine learning models to HuggingFaceSpaces, which lets me send a link easily to Stuart, Robert, the biology dept etc and let them test and play with what I&#39;m doing. This will be very useful as I physically will not need to take my computer over to get feedback on models. | How the project will be examined: by extended report (like a dissertation), a seminar, a supervisors mark (on your effort and from a viva), and also a provisional formative 10 page report. An external supervisor will be picked at random from 4 academics Stuart and Robert pick to read your report. | Stuart said to take the formative report seriously, because feedback on your writing style etc is really important. Your supervisors can&#39;t give feedback on your real report later on due to department rules. | We will meet with Phil and Steve, from the biology department, next week, meeting at the TLC at 10:55. | Stuart has two audio recording devices that use SD cards to record. He said that he will order SD cards. I could use these to collect sound data if necessary. There is a large amount of data already collected, both by the biology department and available online, but if I wanted data collected in a specific way, then I could do it myself. For example, as we have two devices, I could set both of them up to gather the same audio, and use that to analyse out noise. What I could do, is use the same microphones the biology department did, and use them to remove noise from their whole dataset. | On a ambitious note, the biology department has been using community volunteers to look through photos and label animals in them. In a similar way, if this project is really successful, we could get a grant to lend out a specific recording setup to them and get a lot of specific data. | Robert explained some possible project aims relating to biodiversity. For example, if we could classify birds by audio, we could look for birds that shouldn&#39;t be in the UK or in specific areas. Birds that are migrating in the wrong direction, due to climate change or other environmental issues. This could tell us about biodiversity. If we had data of audio collected over months or years, then about how biodiversity is changing over time. I commented that if the biology department also recorded the GPS location of their recording device, it could be possible to map out how the number of specific birds changes in various areas. | I asked Stuart whether any of his other supervision students are working on a similar problem to me. While he has many students, we are all doing varied projects which is interesting but hinders collaboration. For now, there should be a PhD student working with the biology team with I could work with. | I explained that I want my diss to be written in a way it can be understood by anyone with no prerequisite knowledge. This stems from me expressing that the fundamental mechanisms in which machine learning works is actually very simple, it&#39;s high school level maths and can be explained in one or two pages. This approach is good because it will make examination and marking easier. | I explained that for frequent progress updates, I want to continue writing blogs on this website and link it to the teams channel. Right now one blog a week after the week&#39;s meeting is probably a good pace. | We agreed on my plan to spend half my time working directly on the problem, and the other half going through the second fast.ai course starting Tuesday next week. | . I also attended the intro physics project talk this week: . Level 4 contributes to 44.4% of your final year mark. | My other modules, atomslasersqubits and planets&amp;cosmology, have one self assessed problem a week. | Planets&amp;Cos has 80% exam, and 20% essay. Choose a topic and write 1500-words for deadline 2pm Friday 10th Feb, but from disabilities I may be able to get an extension if needed. | Technical support at physics.level4lab@durham.ac.uk to help with small scale jobs, advice, equipment ordering. Could be useful to setup recording devices for me. | There are weekly Python computing drop in sessions at Ph216 at 12-1pm on Mondays and Thursday with PhD students to help etc. Also there&#39;s an online support website. | Assessment: In order. Seminar 5% between 27th Feb-3rd March. Oral exam 25% between 24th April-2nd May. 40% report, 30 pages, 2pm Weds 19th April deadline. 30% supervisors&#39; mark from supervisors&#39; thoughts about my effort and a viva. | Majority of the diss writing happens over Easter. The deadline is 2pm Weds 19th April, a week before 3rd term. | Upload updates to the project on teams to act as a record. It helps with the supervisors&#39; mark. It helps with external examiners to act as evidence between interaction between student and supervisors. Great for diss writing. | Short 10 page interim report for Christmas assessed by main supervisor. | Advice to write reports in LaTeX, although I think I will write mine in Jupyter notebooks for easy code integration like how the fast.ai textbook is written. | Oral exam/Viva, 25 minutes, with an examiner, moderator, supervisors. No formative practice for this. | Formative 10 minute seminar with supervisors after Christmas. | Can borrow equipment from labs if given permission. Might need to do a risk assessment. | . Next Steps: . This week I want to: . Finish fast.ai part 1. Namely the questions and blog posts for the last 2 lessons. | Go and investigate some of the project&#39;s proposal&#39;s references starting with scikit-maad and a CNN classifier. | Ask for a reimbursement for fast.ai part 2 | . Long term plan: . Spend half my time going at the problem directly. The proposal references. I found that fast.ai have an audio library, fastaudio at https://github.com/fastaudio/fastaudio, and online I found several blog posts about people using it to classify birds. https://www.ecosia.org/search?q=fast%20ai%20fastaudio%20bird%20classification&amp;addon=opensearch. Also it would be good to learn some domain specific knowledge, I.E, knowledge about birds and bird songs, specific knowledge that is helpful to inform ML models. | Spend the other half of my time learning more machine learning knowledge, namely, working through the fast.ai part 2 course starting next week. My reasoning is as follows: the second course covers more advanced topics likely highly important for my Masters. For example, how to implement the most recent ML papers into your work, which requires higher knowledge about how to edit code architecture. And further forward, if I want to add explainable AI modules into my work, this knowledge may prove useful. Lastly, there&#39;s likely many useful topics covered that I don&#39;t get know of. | As a side, this style of plan is inspired by Jeremy Howard, the creator of fast.ai. He said that thinks spending half your time working and half your time learning is a great approach for computer science. The combination of real practice while also learning is good. | .",
            "url": "https://exiomius.github.io/Blogs/mp/2022/09/25/MP2.html",
            "relUrl": "/mp/2022/09/25/MP2.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Masters Project First Post",
            "content": "This is the start of a series of posts about my Masters project with the Physics Department at Durham University. To being with, here is the official project description as presented by the department: . Title: Data science for biodiversity loss . Supervisor: Prof Charles Adams. Second Supervisor: Dr Robert Potvliege. Category: General. Type: Computation/Data Analysis/Experiment. . Biodiversity loss due to human action is increasingly creating an existential threat to all live on Earth. In order to take appropriate action we need better data. However biodiversity data is both more diverse and more difficult to accumulate than say climate data. In this project, we shall look at data analysis on bird song. Although, under ideal conditions it is possible to identify different species [1-3]. In a noisy environment, which is more typical, this becomes more challenging. One approach that we shall pursue is to construct time frequency pattern and then use pattern recognition technique to identify particular events [4]. . [1] scikit‐maad An open‐source and modular toolbox for quantitative soundscape analysis in Python, Ulloa et al, Meth. in Ecology and Evolution, 12, 2334 2021 . [2] Multifractal analysis of birdsong and its correlation structure, R Bishal, GB Mindlin, and N Gupte Phys. Rev. E 105, 014118 2022 . [3] Large-scale analysis of frequency modulation in birdsong data bases, D Stowell, MD Plumbley, Methods Ecol Evol, 5: 901 (2014) . [4] New aspects in birdsong recognition utilizing the gabor transform, S HEUER , P TAFO, H HOLZMANN, S DAHLKE, Proc. of the 23rd Int. Congress on Acousitics, Aachen, September 9-13, 2019. . Aim . In essence, the (first) aim of this project is to pick apart bird song from a noisy sound recording and identify birds from it. . In the future other aims may emerge, such as trying to understand the meaning behind birdsong rather than classify their singers. Perhaps even trying to generate birdsong may be an interesting idea to yield some insights. It would be fascinating seeing if/how birds would respond to generated birdsong. . Approaches . The four above referenced papers are probably the best place to start when looking for initial approaches. While I am comfortable with both Physics and Programming, my preference in comfortability and in interest does lean towards the latter, especially when Machine Learning is involved. Having said that, the first, third and fourth reference all are ML based to a degree. . I don&#39;t yet have the prerequisite knowledge to understand at a glance the abstracts of the second and third references. . The first however is an open-source Python module called scikit-maad, which is instantly recognisable by my familiarity with other popular scikit modules. Furthermore, I feel warmly welcomed by the described online documentation and practical examples around it. Lastly, the module highlights its ability to easily integrate Machine Learning Python packages. . The fourth reference details that current approaches convert audio recordings into spectrograms using the Gabor transform, then enter them as images to a CNN for classification. This is a really intuitive ML approach, and one I might try and implement. The paper then details that most approaches focus on finding the best CNN hyperparameters for accuracy, so in contrast the authors attempt to evaluate the parameters for the Gabor transform itself. . All in all, I think my first priority is to investigate and implement the first and fourth references. It&#39;s not ideal not being able to easily understand the other papers, but my reasoning is that having gone through the more understandable references first would yield prerequisite knowledge to go back and understand the others. . Coincidentally, I was talking briefly with my older brother about the project and he commented that the problem is awfully similar to other audio separation problems. Separating out bird song from a noisy forest environment and then identifying them, is similar to separating out instrument sounds from a regular song and identifying them. We both actually have a mutual friend who did his Masters project in the latter. Spotify also appears to have its own development going on for this problem. Looking through how similar these two problems are might prove very useful. . Next Steps . Currently I am still doing the first part of the excellent Fast.ai deep learning online course. As this project is in joint collaboration with the biology department, we are having a full team meeting in a few weeks. . Firstly I will finish off the first part of Fast.ai since it really is interesting and useful for both this project and my future prospects. . What to do next however is a bit less obvious. To work on the aforementioned project references or to continue with the new Fast.ai course: . Fast.ai is releasing the second half of their 2022 course starting this October. From a personal perspective it is really attractive to focus on it because it aligns with my graduate and personal interests. From a project perspective it is attractive to focus on it because of Fast.ai&#39;s fixation on updating and teaching state-of-the-art techniques. The course would provide me with excellently taught tools to tackle the project with. . However, the course would likely be time consuming: it will contain eight weekly lessons, each with a double lecture, programming notebook(s), a textbook chapter, and a question sheet. Having said that, some lessons in part 1 actually didn&#39;t take me an enormous amount of time to complete. Others, like lesson 2, took a very long time. Since part 2 is advanced, I&#39;d imagine it would be considerably harder. . There is a registration deadline (7th October) and payment (175 Australian dollars, about £102). Although, come early 2023, the course will become available online for free. . Between finishing my Laidlaw Internship, doing my part-time job at the university, studying for the other modules of my degree, and of course participating in university life, I have to be efficient with the time I spend for my Masters project. . My current plan is to: Finish Fast.ai part 1. I will prioritise going through scikit-maad. I will however, still register and purchase Fast.ai part 2. If time allows (within my free time and during holidays), I will go through the course. .",
            "url": "https://exiomius.github.io/Blogs/mp/2022/09/25/MP1.html",
            "relUrl": "/mp/2022/09/25/MP1.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Fast.ai Lessons 4 Roundup",
            "content": "Lesson Overview . This lesson was an introduction to natural language processing (NLP). NLP includes tasks such as text classification (e.g. positive or negative sentiment) and text generation (creating new text from a prompt). It appears to be an immensely growing field and as a result certain modules are struggling to keep up to date with the state-of-the-art. A new technique called transformers has recently become popular, and FastAI&#39;s library for example has not been updated with its functionality yet, despite the authors working on it. As a result, this lesson&#39;s lecture and notebooks covers HuggingFace&#39;s transformer module, while the textbook still uses the older FastAI methods. . This dilemma reflects a common issue with growing fields. As the state-of-the-art changes so quickly, one must take care to be flexible and be comfortable with learning many modules and ways of doing things. It doesn&#39;t seem to bad however, because the fundamentals behind the techniques are still not often changed, so learning it once properly will make learning newer techniques quick. Furthermore, updated modules often have really good documentation, tutorials and discussions available. HuggingFace seems to have some brilliant transformer documentation across their webpages and GitHub. Lastly, this &#39;dilemma&#39; actually might be pretty entertaining. Seeing your field grow with changes and improvements and having to learn about them may keep working pretty interesting. In a way it is like also having to be wear the hat of an academic as a part of your usual job role. The variation could be fun. . Notebooks covered . This lesson covered two notebooks,Getting started with NLP for absolute beginners and Iterate like a grandmaster! . &quot;Getting started with NLP for absolute beginners&quot; covers: . How to use Kaggle. Including how to use Kaggle datasets on your own PC and how to submit entries to their competitions. | How to use Pandas and Transformers DataFrames to view data, and get it into the correct format for NLP model training. | How to use Transformers to train and classify. | The difference between training, validation, and testing data. An emphasis has been put on creating good validation data with a quote by Dr Rachel Thomas stating that often a poorly chosen validation dataset results in a disconnect between development and deployment performance. Her article describes this in more detail. | Training data is to train. Validation data is get an idea of generalizable performance, but often it is limited in doing so. This is often either because it hasn&#39;t been chosen prudently enough or because one has accidentally overfitted to it. Imagine a model to predict the price of a stock. Randomly selecting points to be validation data is poor because that is not how the model will be used in practice and is a much easier task. Selecting some amount of further price movement to be validation data makes sense, but your model may overfit to the specific movement pattern of that timeframe. | The Pearson Correlation Coefficient, r, as a metric is discussed. Emphasis is put on firstly trying metrics on datasets to understand them, rather than delving immediately into the maths. Doing so for example yielding the fact that r is really sensitive to outliers. | On outliers. Outliers are important parts of the data and mustn&#39;t be removed without reason. In our housing income and average number of rooms correlation example, there are outliers that could lead to some insights about the data. Perhaps the outliers are from a specific type of house or a specific geographical area. In this case, it may make more sense to use separate models to train on and predict separate clusters of data. | On hyperparameters. Learning rate is the most important in this case. The idea is to find the largest value that doesn&#39;t result in failed training. FastAI provides a learning rate finder to help you, but Transformers does not. | . &quot;Iterate like a grandmaster!&quot; covers: . How a grandmaster Kaggle competitor works. He focused on creating an effective validation set and iterating rapidly to find changes which improve validation set results. These skills carry over to real projects. | For the patent classification, the input is anchor and label is target. In the test data, there are anchors not present in the training data. Thus we should make sure there are anchors in the validation data that are not in the training data. | Pick a learning rate and batch size that fits your GPU. This means picking them so that we can iterate rapidly to test things out. | Pick a reasonable weight decay. | Pick a small number of epochs, like 3, to test with. This is because in practice much of the performance will be made in those. Thus there is no need to run many epochs every time you try a change. If there is not improvement within a few epochs, then your change is likely not very significant. Later on, when you want to more thoroughly evaluate a change, you can use more epochs and cross-validation. | Pick a class to setup your arguments for your trainer. Transformers by default uses one, but FastAI has others. | You need a stable validation accuracy from your epochs to know whether your future changes is making improvements. To know whether your predictions are stable, run the model from scratch a few times, say 3, and check how much it varies. | To make changes easier, create a function to setup tokenization and a function to setup model creation. Then you can pass parameters quickly to create new models. | In this case, previously we tokenised with a special token sep to indicate seperate entities in our input. Simply changing sep to s resulting in a big performance increase. | Instead of using the same special token to indicate separate entities in our input, using different special tokens for each entity could better inform the model that each entity is different. | Simply changing all text to lowercase can often help a little too. | There&#39;s so many things you could try. In this notebook, most of the iteration was done by changing tokenisation, but also playing around with the other parameters might yield better results. However, instead of trying to optimise the factors already present, there are other ideas you can try. Firstly fine tuning your general language model using just patent data. Or using a model pretrained on legal vocabulary instead of general vocabulary. Using a different type of model, not in terms of architecture but a model created for a different task. One of our columns is &#39;context&#39;, which is a code e.g. B7 referring to a patent context. Instead of using the code, we could replace it with a description found online. There&#39;s so many things you can try especially if you think a little creatively. | Remember for the final submission, to train on your validation data as well. | . A subset of this chapter&#39;s questions and my shortened answers are as follows: . What is self-supervised learning? When you don&#39;t use labeled data. You make the model divide the data into input and label itself. Self-supervised learning is often used to train models to be used as a transfer model for a different task such as classification. | What is a language model? A model trained to predict the next word of an input text. | Why do we fine-tune language models? Because the pretrained model wouldn&#39;t have had specifically been calibrated for your specific task. | What is Universal Language Model Fine-Tuning? It is an approach to NLP including transfer models. In order: Find/train a general language model, transfer/fine tune it to make a task specific language model, transfer/fine tune it to make a final model for the task at hand. | How does one need to prepare their data for a language model? Tokenisation, Numericalization and DataLoading must be done. | What is Tokenisation? Tokenisation: Convert the text into a list of words/characters/substrings. The way you do this has to the same throughout all the models you use. If you grab a model online, you have to tokenise your models in the same way. | What is Numericalization? Numericalization: First make a vocab list/dictionary of all the words used in the dataset. They will all be given unique numbers to identify them. Then convert the tokenised text into a list of these numbers. | What is DataLoading? In FastAI LMDataLoader automatically sets the last token in a sequence as a label, as well as other important data preparation processes. | Many details about tokenisation are discussed including special characters, rules, and repeated characters. | Why would a model tokenise some words as &#39;unknown&#39;? Because there is a limit to the number of words/tokens the vocab list/embedding matrix contains. | Why is padding needed for text classification but not language modeling? Because PyTorch&#39;s DataLoader(s) uses tensors that can only store elements of the same type and size. In text classification, the text/documents in tensors are of varying size. In language modeling, they are not. | What is an embedding matrix? I need to spend more time on this because it is a little complicated. It contains a list of vector representations of all the tokens present in the vocab list. | What is perplexity? A performance metric used for judge NLP models. | What is gradual unfreezing? It&#39;s a way of training a model. It is unfreezing one layer at a time. Unfreeze a layer, then training that layer’s parameters, then unfreezing another layer and then train, etc until all layers are unfrozen. | Why is text generation always likely to be ahead of automatic identification of machine-generated texts? Because classifier identification models can be used to create better generation models. And in a way by definition, a better classifier model can only be trained on a better generation model after it has already been released online. | . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson4.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my github repository at https://github.com/exiomius/PDL-Lesson-4 .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/21/Lesson4Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/21/Lesson4Blog.md.html",
            "date": " • Sep 21, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Fast.ai Lessons 3 Roundup",
            "content": "Lesson Overview . In this lesson the mechanisms behind deep learning are explored. Unlike the previous lesson, which was focused on applications and many pieces of new software, this lesson was focused on reading through a textbook chapter and understanding how exactly deep learning operates. . I suppose this knowledge will be required in order to understand how best to create good deep learning models. . The topics covered, briefly . How are images represented in a computer. | How are files in datasets structured. | Python functionality: what is list comprehension, and fact that NumPy/PyTorch are much faster than pure python. | Tensors: a list like structure used in deep learning. Ranks indicating dimensions and shapes indicating the length of each axis. | Loss functions: RMSE and L1 norm. Two loss functions, similar but RMSE penalises larger errors more. | SGD: stochastic gradient descent. A way to make a model learn by updating its weights automatically. | The difference between loss and metrics: Loss is for updating weights, metrics are for human evaluation. | Mini-batches. Each epoch, instead of using one piece of training data to update weights (pure SGD), or using all the training data simultaneously to update the weights (GD), splitting the data into random batches (subsets) of the training data to update the weights. | The sigmoid function: a smooth curve between 0 and 1 used in deep learning as a loss function. It is used because it has a meaningful derivative so allows good weight updates. | DataLoader class: a function to take a dataset and split it into random mini-batches. DataLoaders contains a training a training and validation DataLoader. | ReLU: a rectified linear unit. It&#39;s just a linear function, a line, with negative values = 0. | Activation functions. Also known as nonlinear functions, these are placed between layers of linear functions in neural networks so that the linear functions do not combine into another linear function. | The Universal Approximation Theorem: that two linear layers with an activation function inbetween can approximate any function given the correct weights and enough nodes. In practice however we use more than two linear layers because it works better. | . Much more content was covered in much more detail, including how to program a deep learning model from scratch in Python. . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson3.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository at https://github.com/exiomius/PDL-Lesson-3. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/19/Lesson3Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/19/Lesson3Blog.md.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Fast.ai Lessons 1&2 Roundup",
            "content": "Lesson Overview . The first lesson did well to summarise an introduction to model creation. While it was content I have already covered throughout previous courses, a quick refresher is good. The teaching philosophy behind the course, to learn by doing in conjunction to theory, is excellent in keeping things engaging and enjoyable. The second lesson covered many new software tools and applications very concisely. Although, I had to spend a lot of time troubleshooting various issues with getting things to work on my PC. . The topics covered, briefly . How to create and train a deep learning model in FastAI | How to better filter and amend training data to get better model performance | How to export a ML model as a file | How to load ML models and allow websites to use them | How to save coding repositories online | How to host and create blog posts that include code easily | . This included Kaggle/Google Collab for creating, training and exporting models. Anaconda for Jupyter Notebooks. Visual Studio Code, a programming IDE, to interact with: HuggingFaceSpaces, a website to host models, and GitHub, a website to store one&#39;s code. It took much time and troubleshooting to understand how to install and use these tools, but the result is the understanding of how to quickly produce deep learning models and make them accessible on the internet, as well as produce blogs containing code, with good practices and expandability behind functionality. . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson2.html, which includes a lecture, a notebook, and a set of questions from the course book. My answers can be found on my GitHub repository below. . My machine learning model &quot;NovaOrToast&quot; attempts to classify whether a given cat photo is my older sister&#39;s cat Toast, or my younger sister&#39;s cat Nova. It can be found at https://huggingface.co/spaces/exiomius/NovaOrToast, where it is possible to upload images and get them classified immediately. . My repository for this can be found at https://github.com/exiomius/NovaOrToast The code for the model creation and training can be found at https://www.kaggle.com/code/adnanjinnah/nova-or-toast-model-creator/edit. One must use an online GPU service like Kaggle or Google Collab to train models due to their computational intensity. It should be possible to link said services with GitHub, which would be best to keep a centralised codebase. . The pipeline to upload a machine learning model online . Firstly create, train and export the model using a online IDE such as Google Collab or Kaggle. This is because the training will take up much GPU computation, so it is better to avoid doing it on one&#39;s own machine. | Create a Jupyter Notebook and import the fully trained model. Use a Python module called Gradio to create a local webpage of your model to test if it works on your local web adress. | Convert your Jupyter Notebook into a .py file. | Using HuggingFaceSpaces, create a space for your new model. | Using Visual Studio Code, clone the repository of your new space, then add in your Jupyter Notebook&#39;s, .py file after conversion. | Create a &#39;requirements&#39;.txt including the modules used so that HuggingFaceSpaces can install them as required. | Using Visual Studio Code, use Git to push your update to HuggingFaceSpaces. | After a few minutes, your model will be avaliable on the HuggingFaceSpaces space you created. The progress behind this will be available to see on your GitHub repository under the &#39;actions&#39; tab. | Not included are many many details of how to specifically do these instructions, as there are too many to throughly convey, especially as much time was spend troubleshooting for my specific machine and operating system (Windows). . Things to improve: . A better way to convert Jupyter Notebooks to .py files. It seems to work only with a settings.ini file, but I was not able to create one that works myself, so had to copy one on the course director&#39;s GitHub, which works but I cannot figure out how to place the resulting .py file in the correct folder. | How to upload Kaggle code to GitHub seamlessly. | A way to automatically update Anaconda&#39;s modules. The default Anaconda update prompts update the app but not always the modules I require. For instance, it took me a while to figure out the Python Pillow module wasn&#39;t updated on Anaconda despite it being a fresh installation. | .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/15/Lesson1&2Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/15/Lesson1&2Blog.md.html",
            "date": " • Sep 15, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://exiomius.github.io/Blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://exiomius.github.io/Blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}