{
  
    
        "post0": {
            "title": "FastAI Lessons 4 Roundup",
            "content": "Lesson Overview . This lesson was an introduction to natural language processing (NLP). NLP includes tasks such as text classification (e.g. positive or negative sentiment) and text generation (creating new text from a prompt). It appears to be an immensely growing field and as a result certain modules are struggling to keep up to date with the state-of-the-art. A new technique called transformers has recently become popular, and FastAI&#39;s library for example has not been updated with its functionality yet, despite the authors working on it. As a result, this lesson&#39;s lecture and notebooks covers HuggingFace&#39;s transformer module, while the textbook still uses the older FastAI methods. . This dilemma reflects a common issue with growing fields. As the state-of-the-art changes so quickly, one must take care to be flexible and be comfortable with learning many modules and ways of doing things. It doesn&#39;t seem to bad however, because the fundamentals behind the techniques are still not often changed, so learning it once properly will make learning newer techniques quick. Furthermore, updated modules often have really good documentation, tutorials and discussions available. HuggingFace seems to have some brilliant transformer documentation across their webpages and github. Lastly, this &#39;dilemma&#39; actually might be pretty entertaining. Seeing your field grow with changes and improvements and having to learn about them may keep working pretty interesting. In a way it is like also having to be wear the hat of an academic as a part of your usual job role. The variation could be fun. . Notebooks covered . This lesson covered two notebooks,Getting started with NLP for absolute beginners and Iterate like a grandmaster! . &quot;Getting started with NLP for absolute beginners&quot; covers: . How to use Kaggle. Including how to use Kaggle datasets on your own PC and how to submit entries to their competitions. | How to use Pandas and Transformers DataFrames to view data, and get it into the correct format for NLP model training. | How to use Transformers to train and classify. | The difference between training, validation, and testing data. An emphasis has been put on creating good validation data with a quote by Dr Rachel Thomas stating that often a poorly chosen validation dataset results in a disconnect between development and deployment performance. Her article article describes this in more detail. | Training data is to train. Validation data is get an idea of generalizable performance, but often it is limited in doing so. This is often either because it hasn&#39;t been chosen prudently enough or because one has accidentally overfitted to it. Imagine a model to predict the price of a stock. Randomly selecting points to be validation data is poor because that is not how the model will be used in practice and is a much easier task. Selecting some amount of further price movement to be validation data makes sense, but your model may overfit to the specific movement pattern of that timeframe. | The Pearson Correlation Coefficient,r, as a metric is discussed. Emphasis is put on firstly trying metrics on datasets to understand them, rather than delving immediately into the maths. Doing so for example yielding the fact that r is really sensitive to outliers. | On outliers. Outliers are important parts of the data and mustn&#39;t be removed without reason. In our housing income and average number of rooms correlation example, there are outliers that could lead to some insights about the data. Perhaps the outliers are from a specific type of house or a specific geographical area. In this case, it may make more sense to use separate models to train on and predict separate clusters of data. | On hyperparameters. Learning rate is the most important in this case. The idea is to find the largest value that doesn&#39;t result in failed training. FastAI provides a learning rate finder to help you, but Transformers does not. | . &quot;Iterate like a grandmaster!&quot; covers: . How a grandmaster Kaggle competitor works. He focused on creating an effective validation set and iterating rapidly to find changes which improve validation set results. These skills carry over to real projects. | For the patent classification, the input is anchor and label is target. In the test data, there are anchors not present in the training data. Thus we should make sure there are anchors in the validation data that are not in the training data. | Pick a learning rate and batch size that fits your GPU. This means picking them so that we can iterate rapidly to test things out. | Pick a reasonable weight decay. | Pick a small number of epochs, like 3, to test with. This is because in practice much of the performance will be made in those. Thus there is no need to run many epochs every time you try a change. If there is not improvement within a few epochs, then your change is likely not very significant. Later on, when you want to more thoroughly evaluate a change, you can use more epochs and cross-validation. | Pick a class to setup your arguments for your trainer. Transformers by default uses one, but FastAI has others. | You need a stable validation accuracy from your epochs to know whether your future changes is making improvements. To know whether your predictions are stable, run the model from scratch a few times, say 3, and check how much it varies. | To make changes easier, create a function to setup tokenization and a function to setup model creation. Then you can pass parameters quickly to create new models. | In this case, previously we tokenised with a special token sep to indicate seperate entities in our input. Simply changing sep to s resulting in a big performance increase. | Instead of using the same special token to indicate separate entities in our input, using different special tokens for each entity could better inform the model that each entity is different. | Simply changing all text to lowercase can often help a little too. | There&#39;s so many things you could try. In this notebook, most of the iteration was done by changing tokenisation, but also playing around with the other parameters might yield better results. However, instead of trying to optimise the factors already present, there are other ideas you can try. Firstly fine tuning your general language model using just patent data. Or using a model pretrained on legal vocabulary instead of general vocabulary. Using a different type of model, not in terms of architecture but a model created for a different task. One of our columns is &#39;context&#39;, which is a code e.g. B7 referring to a patent context. Instead of using the code, we could replace it with a description found online. There&#39;s so many things you can try especially if you think a little creatively. | Remember for the final submission, to train on your validation data as well. | . A subset of this chapter&#39;s questions and my shortened answers are as follows: . What is self-supervised learning? When you don&#39;t use labeled data. You make the model divide the data into input and label itself. Self-supervised learning is often used to train models to be used as a transfer model for a different task such as classification. | What is a language model? A model trained to predict the next word of an input text. | Why do we fine-tune language models? Because the pretrained model wouldn&#39;t have had specifically been calibrated for your specific task. | What is Universal Language Model Fine-Tuning? It is an approach to NLP including transfer models. In order: Find/train a general language model, transfer/fine tune it to make a task specific language model, transfer/fine tune it to make a final model for the task at hand. | How does one need to prepare their data for a language model? Tokenisation, Numericalization and DataLoading must be done. | What is Tokenisation? Tokenisation: Convert the text into a list of words/characters/substrings. The way you do this has to the same throughout all the models you use. If you grab a model online, you have to tokenise your models in the same way. | What is Numericalization? Numericalization: First make a vocab list/dictionary of all the words used in the dataset. They will all be given unique numbers to identify them. Then convert the tokenised text into a list of these numbers. | What is DataLoading? In FastAI LMDataLoader automatically sets the last token in a sequence as a label, as well as other important data preparation processes. | Many details about tokenisation are discussed including special characters, rules, and repeated characters. | Why would a model tokenise some words as &#39;unknown&#39;? Because there is a limit to the number of words/tokens the vocab list/embedding matrix contains. | Why is padding needed for text classification but not language modeling? Because PyTorch&#39;s DataLoader(s) uses tensors that can only store elements of the same type and size. In text classification, the text/documents in tensors are of varying size. In language modeling, they are not. | What is an embedding matrix? I need to spend more time on this because it is a little complicated. It contains a list of vector representations of all the tokens present in the vocab list. | What is perplexity? A performance metric used for judge NLP models. | What is gradual unfreezing? It&#39;s a way of training a model. It is unfreezing one layer at a time. Unfreeze a layer, then training that layer’s parameters, then unfreezing another layer and then train, etc until all layers are unfrozen. | Why is text generation always likely to be ahead of automatic identification of machine-generated texts? Because classifier identification models can be used to create better generation models. And in a way by definition, a better classifier model can only be trained on a better generation model after it has already been released online. | . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson4.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my github repository at https://github.com/exiomius/PDL-Lesson-4 .",
            "url": "https://exiomius.github.io/Blogs/2022/09/21/Lesson4Blog.md.html",
            "relUrl": "/2022/09/21/Lesson4Blog.md.html",
            "date": " • Sep 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "FastAI Lessons 3 Roundup",
            "content": "The topics covered, briefly . In this lesson the mechanisms behind deep learning are explored. Unlike the previous lesson, which was focussed on applications and many pieces of new software, this lesson was focussed on reading through a textbook chapter and understanding how exactly deep learning operates. . I suppose this knowledge will be required in order to understand how best to create good deep learning models. . How are images represented in a computer. | How are files in datasets structured. | Python functionality: what is list comprehension, and fact that NumPy/PyTorch are much faster than pure python. | Tensors: a list like structure used in deep learning. Ranks indicating dimensions and shapes indicating the length of each axis. | Loss functions: RMSE and L1 norm. Two loss functions, similar but RMSE penalises larger errors more. | SGD: stochastic gradient descent. A way to make a model learn by updating its weights automatically. | The difference between loss and metrics: Loss is for updating weights, metrics are for human evaluation. | Mini-batches. Each epoch, instead of using one piece of training data to update weights (pure SGD), or using all the training data simultaneously to update the weights (GD), splitting the data into random batches (subsets) of the training data to update the weights. | The sigmoid function: a smooth curve between 0 and 1 used in deep learning as a loss function. It is used because it has a meaningful derivative so allows good weight updates. | DataLoader class: a function to take a dataset and split it into random mini-batches. DataLoaders contains a training a training and validation DataLoader. | ReLU: a rectified linear unit. It&#39;s just a linear function, a line, with negative values = 0. | Activation functions. Also known as nonlinear functions, these are placed between layers of linear functions in neural networks so that the linear functions do not combine into another linear function. | The Universal Approximation Theorem: that two linear layers with an activation function inbetween can approximate any function given the correct weights and enough nodes. In practice however we use more than two linear layers because it works better. | . Much more content was covered in much more detail, including how to program a deep learning model from scratch in Python. . The course page for this sessions is https://course.fast.ai/Lessons/lesson3.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my github repository at https://github.com/exiomius/PDL-Lesson-3 .",
            "url": "https://exiomius.github.io/Blogs/2022/09/19/Lesson3Blog.md.html",
            "relUrl": "/2022/09/19/Lesson3Blog.md.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "FastAI Lessons 1&2 Roundup",
            "content": "The topics covered, briefly . How to create and train a deep learning model in fastai | How to better filter and amend training data to get better model performance | How to export a dl model as a file | How to load dl models and allow websites to use them | How to save coding repositories online | How to host and create blog posts that include code easily | . In order to cover these topics, many new tools were required. This includes Kaggle/Google Collab for creating, training and exporting models. Anaconda for Jupyter Notebooks. Visual Studio Code, a programming IDE, to interact with: HuggingFaceSpaces, a website to host models, and Github, a website to store one&#39;s code. It took much time and troubleshooting to understand how to install and use these tools, but the result is the understanding of how to quickly produce deep learning models and make them accessable on the internet, as well as produce blogs containing code, with good practices and expandability behind functionality. . The course page for this sessions is https://course.fast.ai/Lessons/lesson2.html, which includes a lecture, a notebook, and a set of questions from the course book. My answers can be found on my github repository below. . Links to webpage and code . My machine learning model &quot;NovaOrToast&quot; attempts to classify whether a given cat photo is my older sister&#39;s cat Toast, or my younger sister&#39;s cat Nova. It can be found at https://huggingface.co/spaces/exiomius/NovaOrToast , where it is possible to upload images and get them classifed immediantly . My repository for this can be found at https://github.com/exiomius/NovaOrToast The code for the model creation and training can be found at https://www.kaggle.com/code/adnanjinnah/nova-or-toast-model-creator/edit . The pipeline to upload a machine learning model online . Firstly create, train and export the model using a online IDE such as google collab or kaggle. This is because the training will take up much GPU computation, so it is better to avoid doing it on one&#39;s own machine. | Create a Jupyter Notebook and import the fully trained model. Use a python module called Gradio to create a local webpage of your model to test if it works. | Convert your Jupyter Notebook into a .py file. | Using HuggingFaceSpaces, create a space for your new model | Using Visual Studio Code, clone the repositry of your new space, then add in your Jupyter Notebook, .py file. | Create a &#39;requirements&#39;.txt including the modules used so that HuggingFaceSpaces knows the requirements | Using Visual Studio Code, use Git to push your update to HuggingFaceSpaces. | After a few minutes, your model will be avaliable on the HuggingFaceSpaces space you created. | Not included are many many details of how to specifically do these instructions, as there are too many to throughly convey, especially as much time was spend troubleshooting. .",
            "url": "https://exiomius.github.io/Blogs/2022/09/15/Lesson1&2Blog.md.html",
            "relUrl": "/2022/09/15/Lesson1&2Blog.md.html",
            "date": " • Sep 15, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://exiomius.github.io/Blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://exiomius.github.io/Blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://exiomius.github.io/Blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://exiomius.github.io/Blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}