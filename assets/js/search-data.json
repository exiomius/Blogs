{
  
    
        "post0": {
            "title": "Masters Project 18/10/2022",
            "content": "Meeting: . During the meeting we discussed: . Trying to compete directly with Google or other big research teams isn&#39;t a good direction for a one year project. What would be better is a novel direction of something else. | One such approach would be trying to use stero data instead of mono data to solve the cocktail problem. | Most audio data online is mono, for example on xenocanto, but we could try and get in contact with research groups like the biology team and ask for stero data. | One of the reasons why stero data is rarer, is because it requires a special microphone to collect it with. | Different stero microphones have different properties, such as the width between the the two microphones, and this needs to be accounted for when gathering and analysing data. | We could look at stero data, and then calculate the difference between the first channel and second channel to make a third channel. Then try classifying with one channel, and with all three, and seeing if it helps. | What would be interesting only possible with stero data is trying to find the direction of the birds singing. But this might be impossible, because getting labeled data of that is difficult. | The differences between mono and stero data. Besides having 2 channels, there are differences in time delay and attenuation. I need to look into this more. | Investigating whether it&#39;s possible to convert between mono and stero data. It might be impossible to do exactly because there is information missing within the mono data, particularly because intuitively, stero data you can find direction from but mono you cannot. This is an information problem. | Stuart might order a stero microphone to play around with; Robert is asking whether it is possible to borrow one. | Another novel approach would be trying to use stable diffusion to generate spectrograms. The idea being, if there is a lack of stero data we could synthesise our own. There could even be another model added to correct synthesised audio data to be more like real audio data. | A motivation behind this could be the prevalence of an image classification approach in classifying birdsong. Much research uses CNNs for example. | About the Physics content of the project. There needs to be some Physics for the sake of the external marker and external questions at the viva. | Physics content can be added by investigating how to do the image to audio conversion (or vise versa in the case of stable diffusion), because of the transformations and information problem involved, or the prevalence of linear algebra/maths being involved, and even just in the Physics way of thinking of testing hypothesis and different approaches. | . Problem definition update . From now on, I will refer to the problem of trying to identify birds in a noisy environment as &#39;the cocktail problem&#39;, as in at a cocktail party where many people are speaking and where the environment is noisy, it is hard to tell who is speaking. . A soundscape is going to be defined as an audio file that contains various birdsong in a noisy environment. We are trying to classify birds from a soundscape. . Work to do: . I&#39;ve been thinking about all the things I need to do and learn for this project. Here is an overview of them. It&#39;s honestly a lot of work, and hard to tell how much time each will take until I make more progress. It might take the whole of first term to get a handle on this. . 1. Machine Learning Skills . Using frameworks like fast.ai and transformers isn&#39;t as simple as just using their predefined functions and models to do everything. Learning how to find the best hyperparameters, and good validation sets, among many other things, takes a combination of theory and practice to gain intuition. Jeremy from fast.ai said there is no substitute for practice, and provides a lot of guidance on how to do so. . 2. Machine Learning Theory . As well as using frameworks and models, you have to spend time learning the theory behind how they work. For instance, how the components in a CNN work the way they do. . 3. Framework Skills . Understanding the theory, and then having novel ideas to approach the cocktail problem, I need to then implement these ideas by knowing how to create the new code to do so. This involves learning how to edit frameworks and create your own, covered in fast.ai part 2. . There&#39;s also learning about https://nbdev.fast.ai/ to create frameworks and their documentation. . 4. Data handling/preprocessing/Physics . Learning how to store data, access it, transform it into the right size and format, edit it, add noise to it, interpret it (bird domain information) etc. There could be much work to be done on transforming the audio data. Fourier and Gabor transformers etc. I found a YouTube playlist of guides on this at https://www.youtube.com/watch?v=RMfeYitdO-c. The fourth initial project reference, &quot;New aspects in birdsong recognition utilizing the gabor transform&quot;, focuses on the gabor transform and likely much Physics too. . 5. Custom metrics, creation, and evaluation for models . The biology department have their own interests and goals of what they want from a model. I would need to talk in detail with them about their priorities, e.g. preferences in confusion matrix metrics, in bird species etc. They might want a model to work with data over a few years to spot trends too. . 6. Machine learning explainability and communication . Learning how to implement and create methods and visualisations to communicate why the models are predicting as they do. This is especially important for marking in the final report. . 7. Machine learning maths. . To read and implement the latest machine learning papers, some mathematical knowledge is needed. I am contemplating doing yet another free fast.ai course, Computational Linear Algebra, explained here https://www.fast.ai/posts/2017-07-17-num-lin-alg.html, to help with the maths side of things. . Alternatively or in addition, the book Deep Learning by Ian Goodfellow provides a mathematical backing and Jeremy recommended reading the first 6 chapters of it to help with understanding and implementing maths in papers. . Work done . Practiced Transformers . A list of transformer tasks is at https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb which is quite useful. . In particular for audio classification it details the process: &quot; . Instantiate a feature extractor and a model from the checkpoint name. | Process the audio signal to be classified with a feature extractor. | Pass the input through the model and take the argmax to retrieve the most likely class. | Convert the class id to a class name with id2label to return an interpretable result. &quot; | I went through the HuggingFace transformers documentation and did some of the notebooks to understand them. . https://www.kaggle.com/adnanjinnah/audio-classification-hf-1/ | https://www.kaggle.com/adnanjinnah/audio-classification-hf-2/ | https://www.kaggle.com/adnanjinnah/audio-classification-hf-3/ and they covered the 4 step process detailed above. | . Practiced Trying to attempt BirdCLEF 2022 . It&#39;s well worth practicing attempts for a competition with the goal exactly as my own. After trying fast.ai&#39;s audio module last week, and thinking it is outdated (the GitHub repo hasn&#39;t been updated in roughly 6 months), I decided to use HuggingFace instead. This is mainly due to Jeremy recommending it as an up to date framework, but also because it is used in fast.ai part 2. . With that in mind, I attempted it at https://www.kaggle.com/adnanjinnah/birdclef-first-attempt/. This attempt was writhe with problems. While it was my first time using HuggingFace audio, the number of problems I encountered and issues involved were too much. I did not manage to get any model to work. I spent the entire time just trying to get the data loaded properly for usage. . To summarise: . HuggingFace&#39;s load_model has several different methods to load audio. They all require the data to be formatted in a particular way. I tried all them with no success. | Kaggle&#39;s competition dataset is set to read only for some reason. This makes it so I cannot directly just edit the files to get them right. | I tired simply downloading the dataset and reuploading it to Kaggle but A. this is inefficient and B. won&#39;t work for the unseen test data. | I tried copying over the dataset from the read-only input folder to the editable output folder, but this is also inefficient and even so: | I couldn&#39;t load the copied data using load_dataset&#39;s audiofolder function. I&#39;m not sure why. I have it formatted in the exact way the documentation shows. The issue may be I need to upload the dataset to HuggingFace&#39;s website first, but this has the same issues as the first attempt. | A way to get around having to copy the data, with is also inefficient but would atleast work with the unseen test data is to tell load_dataset the URLs of the audio files. This didn&#39;t work either, because some of the URLs don&#39;t work in the instant load_dataset wants to access them. I couldn&#39;t find a way to tell load_dataset to ignore or look later at these URLs. | I tried using a different method of load_dataset, this one however seems to require the main .csv file to contain the audio files in array format. Because the .csv file contains a path to the audio files instead of their content, I tried using another module, librosa, to create a column in the .csv file containing the audio. This didn&#39;t work, because of an excess memory error. And also, this is very inefficient. | . After extensively trying all methods I could find in the documentation with little success, this entire process took around 10 hours. I found tutorials to help with no luck. For now, I&#39;ve given up on trying to get it to work myself. I need to find some resource online or in person to help. In hindsight, I probably should have done this earlier. . On the bright side, atleast I learn&#39;t a few things from the struggle: . First, how transformers requires a dataset to be formatted in a specific way, and that HuggingFace has a website dedicated to storing datasets in an already formatted way. | Experience in reading through documentation and troubleshooting. | The fact that sometimes URLs don&#39;t work, and that last week&#39;s code had a solution, but I couldn&#39;t implement it into HuggingFace&#39;s load_model. | That different loading methods require paths to audio files or them on the .csv file. | That audio files are stored as a file such as .ogg or as an array. | How librosa is a module to convert audio files into audio files into said arrays. | That memory errors will occur from trying to do too much at once. I could get my last method to work if I figured out a way to split up the data, but regardless this approach is inefficient considering we already have the files so it&#39;s better to find a different method. | How to use os to copy files and folders over, or search and retrieve their file paths. | The fact that, for some datasets like BirdCLEF, there is a metadata.csv file with a column for the paths of the audio files. | That for advanced dataset formatting, for HuggingFace, you can create a .py script to do things exactly as you want. | . Finished fast.ai lesson 9: . This lesson was the first of fast.ai part 2 and a very well taught one. In it, Jeremy described conceptually how stable diffusion, an crazy new image generation model, works. Due to it&#39;s difficulty, the lesson took me a full day to complete, but it was well worth it. The ideas and skills I&#39;m being introduced to and learning will prove really helpful for the project going forwards. Next week, the lesson will focus on programming stable diffusion from scratch, and building on that, how to programme your own custom Python machine learning libraries. This is vital because it would allow me not just to copy other people&#39;s code to solve the cocktail problem, but implement my own ideas and test things, perhaps even at a research level. . My post for lesson 9 can be found at: Need to fix FastPages . Finished CLA lesson 1: . Computational Linear Algebra is a fast.ai course covering linear algebra to be centered around practical applications and algorithms. More info and lesson 1 blog can be found here: Need to fix FastPages . Useful Datasets Found . BirdCLEF 2022 uses data from xeno-carto, implying that last week&#39;s approach to downloading them is a good idea. | I found ESC-50, a dataset of labeled environmental audio recordings at https://dagshub.com/kinkusuma/esc50-dataset, also at https://huggingface.co/datasets/ashraq/esc50. These include sounds like rain, sea waves, animals. | I found that Machine Listening Lab at Queen Mary&#39;s University run a birdsong competition and have many datasets that I could possibly use at http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/. | . Useful Research Tools . Scholarcy summarises research articles. | https://inciteful.xyz/ is good for finding papers. | I was told that Prostudy is useful for keeping resources stored for a dissertation. | . A Similar Thesis . My friend&#39;s friend wrote a thesis similar in aim to mine last year. . Title: Using mel-frequency cepstral coefficients and principal components analysis to classify bird vocalisations based on citizen science recordings. Student Name: Alex Dyfrig Swainston. . I messaged Alex and got a copy, and he said he&#39;s happy to help if I have any questions. . New Ideas: . Here are a few new ideas I had about tackling the cocktail problem. . A big issue is the lack of properly labeled data for soundscapes. The biology department painstakingly handlabeled some soundscapes, but it is a difficult and time consuming task that even great ecologists struggle with. What if there was a way to create our own soundscapes that are already labeled? For instance, we have plenty of data from xeno-canto of individual bird songs with varying amounts of noise. What if I also found some audio files of forest environments, and I created a model to combine xeno-canto bird songs with these to imitate a real soundscape? This way, I could create an endless amount of soundscapes to train on, and the birds within them would be labeled! . To create a soundscape: . I could download bird song(s), | Cut out various parts of them, e.g. if it&#39;s 3 minutes long, I cut out random intervals of 20-30 seconds to imitate the bird moving or other sounds overpowering their song, | Randomly vary how loud the bird songs are, | Add in enviromental sounds like a forest soundscape (but being careful there are no birds present!), | Use a noise function, (which is used in stable diffusion), to randomly add noise. Alternatively, find a way to make a model that can generate real noise that is recorded by microphones and use that. | . I could put multiple birdsongs in the same artifical soundscape, and even make them overlap, but I also need to be careful that perhaps I should make the birds singing be realistically in the same environment. I mean I shouldn&#39;t put two birds together that geographically would never meet, or two birds that never sing at the same time of day, or in general enviromental sounds that don&#39;t match the birds present. . Another idea is to add geographical data somehow to the dataset. Perhaps with another input for a satellite image to help. .",
            "url": "https://exiomius.github.io/Blogs/mp/2022/10/13/MP4.html",
            "relUrl": "/mp/2022/10/13/MP4.html",
            "date": " • Oct 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Masters Project 11/10/2022",
            "content": "Meeting: . During the meeting we discussed: . How to introduce more Physics into the project. This would be done namely using stereo audio, which is audio recorded in 3 dimensions. The idea is that using the Doppler effect and other Physics concepts, we could come up with a better audio collection technique that would make the model classification better. However, it may take a lot of time and effort to understand how to setup multiple microphones and analyse their data, given that there are features such as their synchronisation, batteries, and calibration. In total we have around 90 microphones? Because stereo is 3D, we could use it to classify where a bird singing is, and multiple birds singing. Would also need to account for audio reflecting off surfaces. | A previous masters student at Durham, Stuart, worked with the biology department to create his thesis which may contain useful domain knowledge for this project. It can be found at http://etheses.dur.ac.uk/11481/. He is now doing a PhD in Scotland and his further work may be worth looking into, or he may be worth directly getting in contact with to collaborate. | The biology department have published a paper 3 years ago about using a ensemble models to classify the cocktail problem. While the models and approach may be outdated, the data preprocessing steps might be similar to what I would have to do now. | I would have to have a look at the collected datasets to get an idea about their properties and train accordingly. Some of their data was painstakenly hand labeled. | For humans, some bird song is easier classified by image, and some by sound. I suppose this is why ensembles are a good approach. | For the aims of this project, I would need to define a custom metric for the biology department&#39;s needs. For example, if we really want a certain species of bird, we could penalise false positives of it very harshly. | About volunteer interaction: some birds are really hard to classify that even professors struggle. If you were to have a volunteer system, you should have a delegation system to assign hard tasks to experienced users. | Because humans have to listen to the same audio file multiple times to recognise birds, I wonder if we can train or apply a model this way. | Training on bad quality data, which xeno-canto labels, might be a good idea to improve generalisability. | Some bird species have different times at which they sing in the day, so it could be worthwhile including this in the training data somehow. | An interesting problem is say, looking at how the frequency of a rare species changes over time in a given area. Since we have a dataset collected over 3 years, this in theory would be possible. | . Problem clarification . There are two separate and related problems for classifying birdsong: . The easier problem, trying to classify birds from an audio recording of their song in a vacuum. | The harder problem, trying to classify birds from an audio recording of their song in a noisy environment. | . My Masters project is attempting the latter problem. However some testing and knowledge of the former problem is worthwhile regardless. . Work done . Practiced CNNs and the easier classification problem: . Because this project will use CNNs, I revised the code for a basic image classifier at https://www.kaggle.com/adnanjinnah/bird-image-classifier/. | More interestingly, I found a blog post online https://medium.com/mlearning-ai/bird-sound-calssification-using-fastaudio-fastai-49eea9b5953a that attempts to solve the easier classification problem. It uses the intuitive classification approach present in one of the project&#39;s proposal references: convert the audio into an image and use a CNN. The post also shows how to download birdsong audio directly from https://xeno-canto.org/?language=en, a volunteer website dedicated to collecting wildlife sounds. I figured out how to download and label birdsong audio of different species. My code is at https://www.kaggle.com/adnanjinnah/birdsong-easy-1-0/ | . Did research about modern approaches: . There is a small google research group with a webpage at https://bird-mixit.github.io/ They have published a paper &quot;IMPROVING BIRD CLASSIFICATION WITH UNSUPERVISED SOUND SEPARATION&quot; at https://arxiv.org/pdf/2110.03209.pdf. They have not yet released their classification model online, but have released their sound-separation model at https://github.com/google-research/sound-separation/tree/master/models/bird_mixit. . Furthermore, in their paper they talk about a Kaggle competition, BirdCLEF, at https://www.kaggle.com/competitions/birdclef-2022/overview hosted by the Cornell Lab of Ornithology with a $10,000 prize pool. The competition is recent, ending four months ago. While the google team&#39;s results are better, this competition contains the code of the participants, including the winner, as well as discussion threads about them. . Reverse engineering the competition code may be invaluable to my Masters. To do so however, knowledge from fast.ai part 2 will likely be required as well as a lot of time and effort. Thankfully, Kaggle allows you to post your models and see how they would compare on the leaderboard, so I can continuously try different approaches and evaluate how good they are. . An important point is that the google team nor Kaggle competition use accuracy as their final metric. While accuracy initially may seem intuitive to understand, there are many issues with it. That&#39;s why the competition used a metric called F1, which is explained by deepmind at https://deepai.org/machine-learning-glossary-and-terms/f-score, and another post about interpreting it at https://inside.getyourguide.com/blog/2020/9/30/what-makes-a-good-f1-score. . Finished the fast.ai part 1 course lectures, questions, and blog posts: . The last two lessons I completed this week can be found at https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson7Blog.md.html and https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson8Blog.md.html . Although I&#39;ve technically finished the course, the knowledge hasn&#39;t been properly internalised until I spend more time creating models and testing things out. For this reason, the course creator Jeremy says that it&#39;s entirely normal to finish the course and spend months afterwards revising and practicing it. For now though, it should be enough to start the second half and continue to learn as I practice. . Kaggle Competitions: . In theory, the biology department could put up a Kaggle competition for their own exact needs with/without prize money and see what users submit. This would however still require knowledge about how to upload their data, create correct training, validation, and test data, set the correct metrics, interpret the data, and check that submissions are valid. .",
            "url": "https://exiomius.github.io/Blogs/mp/2022/10/06/MP3.html",
            "relUrl": "/mp/2022/10/06/MP3.html",
            "date": " • Oct 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fast.ai Lessons 8 Roundup",
            "content": "Lesson Overview . This is the last lesson of fast.ai part 1. As of writing, fast.ai part 2 starts next Tuesday, and I have already registered for it. I&#39;m very excited for the second half of the course, and really thankful that things fell right into place with my Masters project so that I can dedicate the time and effort towards taking it seriously. . This lesson covers embedding matrices like the previous lesson. The emphasis on embedding matrices makes me believe they are quite important. CNNs are also covered, and again like basic neural networks, they end up fundamentally being pleasantly and surprisingly simple. Lastly Jeremy gives some advice about what to do before the second half of the course. . The topics covered, briefly . More about embedding matrices: about inference parameters, PCA and embedding distances. | A combined approach of a dot product model and a neural network is the best for collaborative filtering. | Transferring the embedding matrix from a neural network to other models can give a nice performance boost. | How CNNs work for image classification: how convolutions are made with filter matrices, how these filter parameters are found with SGD, what stride is, max/avg pooling, how dropout can improve generalisation. | Channels refer to both the input data, but also to the number of activations per grid after a convolution. This second definition is also how features and filters are defined. | What padding is: to avoid losing data on edges. | What stride is: stride 1 is making the conv layer the same grid size as the previous; stride 2 is skipping over. | Why we double channels/filters/features after a stride-2 conv layer. | Refactoring is defining functions for your neural network layers to state its parameters explicitly. E.g. defining a function for convolution layers. | What is a &quot;receptive field&quot;? The area of the image involved in the calculation of (convolutional) layers. | Various visualisation plots to understand how the training process is going. | About learning rates: the benefits and drawbacks of both high and low learning rates. 1cycle and cyclical momentum as smart techniques to modify your learning rate dynamically. | Why activations near zero are problematic. Most obviously, having a final layer of just 0s has no information and so is useless for classification. | Batch Normalisation as a technique to lessen the number of activations near zero. | . Lecture Notes . Jeremy is back to the previous collaborative filtering notebook: | For the embedding matrix, PyTorch keeps track of neural network parameters/weights for you. | movie_bias is the interference parameter we created in order to add bias for movies, user_bias is the one we created in order to add bias for users. | How does our model work? To predict, it trains as normal on the input ratings and output movie ratings, but then we add meaningful interference parameters, movie_bias and user_bias for each movie and user respectively, to adjust with domain specific knowledge. Some movies just everyone likes, so are biased, some users just like every movie, so are biased. | Visualising embeddings: Plotting a principle component analysis of PCA component 1 and 2, gives a compressed view of how our latent factors affect eachother. It gives us a graph to interpret how the top latent factors are related to eachother. Domain specific knowledge would tell us what there PCA components/inference factors are, and we can try and understand why they relate the way they do. | Embedding distance: calculate how far apart each embedding vector is from a specific movie, aka how similar each movie is compared to that movie based on the latent factors. | Using deep learning for collaborative filtering instead: We use a neural net to try and enter the missing values in the matrix. | We use fast.ai to try and find the embedding matrix side/number of latent variables. Fast.ai does it based on Jeremy&#39;s intuition, there&#39;s not a easy way to know how many we should use, although weight decay might give some leeway in having too many. | In practice, a combined approach of a neural net and dot product approach is best for collaborative filtering. | In collaborative filtering, often a small number of users and movies overwealm the rest. For, anime some users watch so much anime compared to other users, and this makes predictions for normal users biased towards predicting for these enthusiastic users. It&#39;s a higher level task to try and resolve this. | Embedding in NLP: an embedding matrix is just a matrix of latent variables for every word. And as before, the embedding distance calculates the embedding distance between a word and other words, telling us how similar they are based on latent variables. | For tabular data, using a tabular model/learner for it, creates and uses an embedding matrix. | Collaborative filtering, NLP, tabular data neural nets, all also use embedding matrices. | Using embedding, latent variables, may be a substitute to doing a lot of feature engineering. | Create and train a neural net, take its embeddings and use it with other models like random forests, it can give a nice performance boost. This is akin to letting the neural network find relationships in the data, and using those relationships to boost another model&#39;s performance. | One latent variable found to help predict retail store sales was distance in real life. Actually the embedding matrix&#39;s distances for that latent variable matched the distances in real life! It managed to learn that real world distance was important for predicting retail sales and reflected that! Latent variables can find amazing relationships about the data! | Jeremy then goes on to cover how CNNs function: | Convolutions: A CNN is similar to the neural networks before, but for computer vision they have a particular difference: they can separate out the horizontal and vertical lines of images into convolutions. | How? It has a filter matrix, say a 3x3 one, and moves around, dot producting and RELUing each 3x3 subset matrix of the original image, producing a smaller image. This is called a convolution. | Why does this work? Because of the values in the filter. E.g. we used a 3x3 matrix with the top elements = 1, middle elements = 0, bottom = -1. The dot product of this will only give the highest value when there&#39;s horizontal image in the top row, anything in the middle, and no image in the bottom, aka whenever there&#39;s a horizontal part of an image! In vertical parts, the bottom row will cancel out the top and produce 0s, aka the convolution layer will have nothing. | The 3x3 matrix is called a filter. | We them use another filter, but this time with two filter matrices, one for our vertical layer and one for our horizontal layer. We do this to combine their features. | In practice, if we wanted to use a CNN to find the best way to make convolutions to classify an image, we wouldn&#39;t know what filter matrix values to use. Previously we just hard coded for a vertical and horizontal convolution, but we want the CNN to find more complex and useful convolutions. | The way we find these filter matrix/kernel values is to set them as parameters and use SGD to optimise. In our example, it will find the good kernel parameters and thus convolutions for digit classification. | Nowadays we do a stride convolution, e.g. stride 2, to skip values of the original image. This reduces the grid size by 2x2. The grid size is not reduced by the kernel size, it is reduced determined by the stride we set. | When we&#39;re done with stride convolutions, with about a 7x7 image at the end, we do an average pool: we average the final values (called activations). | A max pool instead finds the max value of each activation rather than averaging them. | Say we&#39;re trying to identify a picture of a bear. If the bear is a small part of the picture, max pool is much better than avg pool for classification. By checking each end activation for a bear, rather than having one prediction of bear or not bear for all the activations combined, it can spot the bear&#39;s small presence. | Depending on how you want your model to work, you should pick max or avg pool accordingly. fast.ai does this for you, it actually does an average of max/avg pool and tries to find the best for you. | All the kernel multiplies done for a convolution layer is mathematically equivalent to just one big matrix multiplication. | Dropout: we can add a dropout mask which can improve generalisation. | We multiply the mask by our filtered image before we do convolutions to delete parts of it. Higher dropout means the image is harder to see as there is less of it. | The motivation is as follows: a human is able to look at a image with pieces missing and still classify it. A model should be able to as well. If we use dropout, perhaps it forces the model to learn more fundamental features about our images, more resilient and generalisable ones. | There are many more activation functions/activations than ReLU, but in practice it doesn&#39;t make a huge difference so it&#39;s not worth working on it too much. | Jeremy then advices a few things to do before the second half of the course: | Read the book Meta Learning: How to Learn Deep Learning. | Watch the videos again and code and experiment as you go. | Spend time on the forums. | Get together with others to study together. | Build projects. | . Questions . What is a &quot;feature&quot;? | . A transformation of the data which is designed to make it easier to model. Feature engineering is just making new transformations. For example, in the titanic dataset, we could make a new feature, the number of family members a person has, and this could help the model&#39;s predictions. . Write out the convolutional kernel matrix for a top edge detector. | . -pass- . Write out the mathematical operation applied by a 3×3 kernel to a single pixel in an image. | . I&#39;m confused, a kernel doesn&#39;t apply to a single pixel, a 3x3 kernel is applied to 9 pixels, albeit centered on one. The maths for this is just a dot product. . What is the value of a convolutional kernel apply to a 3×3 matrix of zeros? | . 0 . What is &quot;padding&quot;? | . The sides of the image will have the kernel try and use pixels out of bounds. We don&#39;t want to simply lose these sides, so we apply padding, usually just 0 pixels so the kernel has values to use. . What is &quot;stride&quot;? | . Stride-1 is just simply applying the kernel to a centered pixel, then moving one pixel away and doing it again. Stride-2 is moving the centered pixel two pixels away every time. Stride-1 is useful to add convolutional layers without changing input size, noting that it&#39;s the stride, not the kernel size, that affects the convolution layer&#39;s dimensions. Stride-2 is useful to decrease the size of our outputs. . Create a nested list comprehension to complete any task that you choose. | . Nested loops follow the syntax: print(((i for j in range(1,5)) for i in range(1,5))) which is just for j in range(1,5): for i in range (1,5): print(i) print((i for i in list1 for j in list2)) . What are the shapes of the input and weight parameters to PyTorch&#39;s 2D convolution? | . input:: input tensor of shape (minibatch, in_channels, iH, iW) weight:: filters of shape (out_channels, in_channels, kH, kW) iH and iW are just the height and width of the image. kH,kW are just the height and width of the kernel. in_channels and out_channels are just the number of input and output channels. . What is a &quot;channel&quot;? | . A channel is a single basic colour in an image. RGB images have 3 channels, red green and blue. I imagine a 3 channel image like 3 sets of pixel maps, each for a channel, or one pixel map with 3 separate values we can operate on. However channels don&#39;t just refer to the input data as described, but also to the number of activations per grid after a convolution. This second definition is also how features are defined. . What is the relationship between a convolution and a matrix -multiplication? | . A convolution mathematically is just a special matrix of the kernel&#39;s values, multiplied by the pixels, with a bias matrix term added. I.E. kM + b . What is a &quot;convolutional neural network&quot;? | . For a neural network, when we use convolutions instead or in addition to linear layers. . What is the benefit of refactoring parts of your neural network definition? | . Refactoring is defining functions for your neural network layers to state its parameters explicitly. E.g. defining a function for convolution layers. Refactoring makes it much less likely you&#39;ll accidentally make errors in your architecture and also makes it easier for your user to understand how your layers are constructed. . What is Flatten? Where does it need to be included in the MNIST CNN? Why? | . The final Conv2d layer has a output tensor shape of 64x2x1x1, but we need to remove the extra 1x1 layer to get 64x2x1, so we use flatten to do so. This is similar to squeeze in PyTorch. . What does &quot;NCHW&quot; mean? | . Our input&#39; shape is 64x1x28x28: batch,channel,height,width. Meaning a batch of 64 one channel (colour) images of 28x28. NCHW refers to this. . Why does the third layer of the MNIST CNN have 77(1168-16) multiplications? | . The third layer output has the shape 64x16x4x4, so has 16 channels, and so has one bias for each channel. The input shape from the previous layer is 64x8x7x7. For each input pixel (7x7), we multiply by the number of parameters minus the bias weights (1168-16). . What is a &quot;receptive field&quot;? | . The area of the image involved in the calculation of (convolutional) layers. For example, if we select a pixel on the second convolutional layer, we can see the pixels used to calculate it in the first layer, and then further still the pixels used to calculate those in the original image. . What is the size of the receptive field of an activation after two stride 2 convolutions? Why? | . Mathematically, if you apply a 3x3 kernel with 2 stride to a 7x7 area, then do it again, you get one pixel left. . Run conv-example.xlsx yourself and experiment with trace precedents. | . -pass- . Have a look at Jeremy or Sylvain&#39;s list of recent Twitter &quot;like&quot;s, and see if you find any interesting resources or ideas there. | . Jeremy retweeted a kaggle competition where the top teams and winner used fast.ai for their image classification! . How is a color image represented as a tensor? | . A rank 3 tensor. e.g. (3, 1000, 846), 3 colour channels, 1000x846 image size. You then can go into the 3 colour channels and print their individual pixel maps. . How does a convolution work with a color input? | . A convolution takes an image with a certain number of channels, and outputs an image with a different number of channels. For a channel 3 image, we have 3 different kernels, and apply them, then add the result together. . What method can we use to see that data in DataLoaders? | . dls.show_batch . Why do we double the number of filters after each stride-2 conv? | . A stride-2 conv halves the grid size from 14x14 to 7x7. Say the original image was 14x14, then the convolution would be only 7x7. We double the number of filters to avoid this. Filters are just channels, also called features. So say we apply stride-2 to a 14x14 3 channel image, we then end up with a 7x7 grid size with 6 channels. . Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)? | . Neural networks can only create useful features if the number of outputs from them is lower than the number of inputs. It has to condense information to create useful features. That&#39;s why we use a large kernel, because it keeps the number of outputs meaningfully lower than the number of inputs, particularly because if we double the number of filters each time we have a stride-2 layer, we&#39;d be using nine pixels to calculate eight outputs. . What information does ActivationStats save for each layer? | . It records the mean, standard deviation, and history of the activations for each layer, so that we can look into our model&#39;s training process and improve it. . How can we access a learner&#39;s callback after training? | . learn.activation_stats.plot_layer_stats(0) prints out useful plots of the first (0) layer for us. . What are the three statistics plotted by plot_layer_stats? What does the x-axis represent? | . The mean, std, and % of activations with a value near 0. x-axis is just the frequency (number) of activations. . Why are activations near zero problematic? | . We don&#39;t want activations to be 0 or near it. Multiplying by 0 gives 0, which means if a early layer has some 0 activations then latter values will too. Multiplying by 0 means we have computation occuring that doesn&#39;t do anything. If our final layer is just 0s, then it&#39;s not very useful to classification since it&#39;s just empty. . What are the upsides and downsides of training with a larger batch size? | . Larger batches have a more accurate gradient to update the loss with, although they also incur fewer batches per epoch, with means the model weights are updated less frequently. The latter can result in slower training, but it depends on your GPU and other factors. . Why should we avoid using a high learning rate at the start of training? | . Because the initial weights were initialised randomly, a high starting learning rate could result in the training being ruined at the start very quickly by going in a very wrong direction. Also: a lower learning rate throughout would result in ending training at a local minima, while a high learning rate would skip over them. . What is 1cycle training? | . To avoid the previous problem, we want to start with a low learning rate, but when the training has settled some, we want to increase the learning rate to speed up the training, but near the end we want a low learning rate again to avoid accidentily skipping over loss minima. . This is where 1cycle training comes in: it splits the learning rate into a warmup and annealing phase. The former starts low and increases to a maximum, and the latter decreases from the maximum back to the lowest. . What are the benefits of training with a high learning rate? | . A high learning rate trains quickly and can avoid being stuck in local loss minima. . Why do we want to use a low learning rate at the end of training? | . Because we want to converge on a final loss minima, but a high learning rate would skip over it. . What is &quot;cyclical momentum&quot;? | . Momentum is when the optimiser goes in the direction of the gradients as usual, but follows the direction of the previous loss updates. . Cyclical momentum is varying momentum in the opposite direction of the learning rate. When we are at high learning rates, use less momentum, when at low learning rates, use more momentum. . What callback tracks hyperparameter values during training (along with other information)? | . plot_sched . What does one column of pixels in the color_dim plot represent? | . Each vertical slice of the color_dim plot, the plot with the colours changing, represents the histogram of activations for a single batch. . So looking at colour_dim shows us how the activations in the CNN layers shows as training progresses. More specifically, as each vertical line represents the activation histogram for each batch, we see how the activations change after each batch. . What does &quot;bad training&quot; look like in color_dim? Why? | . Dark blue is bad training, at the start all activations are zero. Yellow is better, as there are near-zero activations instead of just zero. But we see over training it gets better as the colour changes so we have less zero activations! But our training is bad because it oscillates in colour, it doesn&#39;t just get better. . What trainable parameters does a batch normalization layer contain? | . For our model to work, we need to fix the initial large zero/near-zero activations and maintain not so many during training. Batch normalisation is a solution to this, it has two parameters, gamma and beta to do so. . What statistics are used to normalize in batch normalization during training? How about during validation? | . During training we normalise the training data with the mean and standard deviation. During validation we instead use the running mean of the stats calculated during training. . Why do models with batch normalization layers generalize better? | . We&#39;re not entirely sure yet but probably because each batch adds some more randomness to the training process. Each mini-batch will have a somewhat different mean and std from the others, so the activations will be normalised differently each time. Thus we force the model to cope with with variations and it improves generalisability. . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson8.html, which includes a lecture, notebooks, and a set of questions from the course book. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson8Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/27/Lesson8Blog.md.html",
            "date": " • Sep 27, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fast.ai Lessons 7 Roundup",
            "content": "Lesson Overview . This lesson covered some more notebooks about how to tweak models to archive better results, including how to get around GPU memory constraints. It also covers how to make a model to predict multiple values, for example classification of multiple categories. Collaborative filtering is detailed in depth for the textbook chapter. . The topics covered, briefly . How to get around GPU memory constraints using gradient accumulation. | Using ensembles of different architectures. | How to make a model predict more than one dependent variable. | Cross entropy loss, mathematically. | Collaborative filtering: what it is, how to create it from scratch. | Collaborative filtering and recommender systems. | Embedding matrices. | PCA. | Positive feedback loops. | The bootstrapping problem. | Weight decay, avoiding overfitting, loss functions. | . Lecture Notes . The lecture covers what is inside a neural net and tweaks to the basic neural net we constructed previously. . Going through road to the top, part 3: . A larger model has more parameters so can find more features, but the problem is that it takes GPU memory that isn&#39;t as flexible as CPU memory. | How to use as large a model as you like without worrying about memory. For example, Kaggle has 16Gb GPUs. | You can first find out how much memory a model uses. What&#39;s important is that training for longer does not actually require more GPU memory. | Gradient accumulation is how: Run smaller batch sizes, but modify them as to act and train as if we were using the same normal batch size for all the training data. | Gradient accumulation results are identical to using a higher memory GPU for certain models. It is for convText and Transformers (NLP). If a model uses batch normalisation, then it won&#39;t exactly, it will have different results, but probably still good ones. | Pick a batch size that fits your GPU memory, and generally higher and a multiple of 8 is better. Generally (not always) if you double batch size, half your learning rate. | We can use ensembles of good models of different architectures, and get even better results. Furthermore, we can add in bagging too to train them on different sets of the training data. | At the start it may feel random as to why certain approaches/models are better, but over time as you develop intuition, it will feel be less random and more systematic. | Generally, it makes sense to iterate on small models then switch to large models, but there&#39;s a better way of ensuring this performance converts correctly. This is covered in the second course. | . Going through road to the top, part 4: . We want a model to now predict two things instead of one I.E, two dependent variables instead of one. E.g. from a rice photo, the type of rice (10 types) and the disease it may have (10 types), so there are 20 categories. | This requires an understanding of making custom loss functions and a deeper look into how cross entropy loss works. | Make a learner just for the first dependent variable, disease, and create a specific metric function for it. | Cross entropy loss: Jeremy states it is really important to understand and so goes into the maths using a separate excel sheet. He tries to predict if a image is a cat,dog,plane,fish or building, so there are 5 classification categories. It outputs 5 numbers (relating to probabilities of each category). CEL first finds the softmax value for each of them. Then it compares the actual value (1 for the correct category, 0 for not) to the softmax value for each category. It multiplies the log of the probability prediction for the correct category by the actual value. | Further info: https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/ | Binary cross entropy is just cross entropy for 1 category: is a cat or not. Careful here, it&#39;s not for 2 categories e.g. cat or dog. | The loss functions in our python environment has two types. The F function type and the nn class type. The latter has more parameters to play with. | Change the last node outputs to be the number of categories predicted instead of the usual 1 for classification. | You encode the loss function for the model to know what/how many categories to predict. You sum loss functions for multiple category types and their sub categories. | This new model, that can predict 20 categories, actually is better than a model that just predicts disease type! This is because the training to do other types of predictions helps. Sometimes this approach is better, sometimes not! | . Collaborative Filtering Deep Dive: . Collaborative filtering is a key part of recommender systems. | We use the Movielens dataset of movie ratings with 3 columns: UserID, MovieID, and rating. | Imagine a matrix of the users and their movie ratings. There are missing values for unrated/unseen movies. Collaborative filtering is just trying to fill in these missing values to complete the matrix. | The problem is predicting how a user will rate an unrated movie for them. We want to match up the user&#39;s movie preferences with the movie&#39;s features to predict this. | But we don&#39;t know their preferences and the movie features, these are called latent variables. We only have their ID, their previous ratings, and those of other users. We can however infer a user&#39;s preferences and a movie&#39;s features from this data. | Let&#39;s assume there are 5 latent factors, say like for a movie, it&#39;s genre, length etc, we don&#39;t set these, we calculate them and then can try and interpret what they are. | On choosing the number of latent factors, its hard. Fast.ai has a function to calculate this based on Jeremy&#39;s intuition, but you can play around too. | Use SGD to optimise these latent factors after we set a loss function. | What is embedding? Just looking something up in an array. An embedding matrix is the array that is looked up. Matrix multiplication in an embedding matrix is the same as looking up index values in a list say as a function in excel. Think about a dot product with a one-hot encoded vector, it just returns the value you&#39;re looking up. | We then cover how to create a collaborative filtering model from scratch using python, PyTorch class definition, and features. | We create a DotProduct class to define embeddings and looking up values for UserIDs and MovieIDs. | Some of our user rating predictions can greater than 5, the maximum. Take our predictions and squish them with a sigmoid to fix this. | We noticed that some users just relates all movies highly, while some users have a range of ratings. Let&#39;s incorporate this into our model predictions. To do so, we make another inference variable, a movie bias and a user bias, reflecting that for movies, they tend to be especially related well or badly, and that for some users, they can rate all movies generally as good or bad. | It&#39;s not covered, but I think we could try and cluster users instead to try and incorporate user and movie types/preferences? | We can use L2 regularisation (weight decay), to avoid overfitting. This adds the sum of the square of the weights to the loss function. This also solves the issue of having useless interfered variables, because they won&#39;t contribute. I suppose this makes getting the exact number of inference variables less important. | In fast.ai, usually defaults are good, but for tabular data, it&#39;s hard to know good defaults, so it&#39;s good to test yourself. | . Questions . The questions from now on will be a part of the blog post rather than uploaded to the lesson&#39;s GitHub repo. . What problem does collaborative filtering solve? | . If you had a matrix of users and their ratings of movies, there are unrated movies so empty spaces. Collaborative filtering attempts to make predictions to fill these spaces. . How does it solve it? | . It uses the data we have to create/infer latent variables, like movie genre or user preferences, to use to predict. . Why might a collaborative filtering predictive model fail to be a very useful recommendation system? | . A recommendation system might be better if it could tell users why they are being recommended certain movies. If we find it difficult to understand what the latent variables mean, this could be hard. The system also can&#39;t act as fluidly as a human and recommend things with incredible movie domain and user preference knowledge. For example, if I asked at a bookstore for a recommendation, they could cater very well to my individual preferences and find a niche great book for me. A collab filtering model could struggle to do this, even with many interference variables. . What does a crosstab representation of collaborative filtering data look like? | . We first show the matrix of userID and movieID detailing the user ratings. Then behind userID and movieID, we show all the values of our latent variables for them. In our example, 5. This is a neat view of for each prediction, the userID and movieID for it, and the 10 total latent variables for it. As we train our model, we can see both the prediction and 10 latent variables change. We use SGD to optimise as usual. . Write the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!). | . -pass- . What is a latent factor? Why is it &quot;latent&quot;? | . A latent factor/variable is a factor not present in the dataset that we infer in order to aid predictions. It&#39;s latent because we have to infer it from the data. . What is a dot product? Calculate a dot product manually using pure Python with lists. | . A dot product is just multiplying each element in a matrix together and adding them up. e.g. (1,2,3).(1,2,3) = 1 + 4 + 9 = 14. . What does pandas.DataFrame.merge do? | . We can merge two columns together if we want. In this case, as humans we want to see not just movieIDs but their titles. . What is an embedding matrix? | . It&#39;s actually not so complicated or scary. An embedding matrix is just a matrix we multiply a one-hot-encoded matrix with. Say we have a one-hot-encoded matrix, if we dot product it with an embedding matrix, it will only return the values we want. Essentially, this is just a quick way to look up values in the embedding matrix. The embedding matrix itself contains the user latent factors and the movie latent factors. . In concrete terms, say we want to make a prediction for user 32&#39;s rating of movie 45. We get make a one-hot-encoded vector and dot product it by the embedding matrix to get user 32&#39;s latent factors and movie 45&#39;s latent factors. Then with our latent factors, we can make our prediction. . What is the relationship between an embedding and a matrix of one-hot-encoded vectors? | . Explained previously. The one-hot-encoded vector simply picks out which embedding matrix values to access/look up. . Why do we need Embedding if we could use one-hot-encoded vectors for the same thing? | . It uses a lot more memory and time. . What does an embedding contain before we start training (assuming we&#39;re not using a pretrained model)? | . They are initialised to random values, so nothing. Just like weights in any model, after training they are more meaningful. After training, they represent something about inference factors and predictions. In NLP, between words&#39; relationships to eachother. . Create a class (without peeking, if possible!) and use it. | . -pass- . What does x[:,0] return? | . The first column. . Rewrite the DotProduct class (without peeking, if possible!) and train a model with it. | . -pass- . What is a good loss function to use for MovieLens? Why? | . Movielens contains movie ID, user ID, and numeric ratings. We use MSELossFlat as our loss function. . It&#39;s fine to use MSE for classification. In fact, there for classification, cross-entropy and MSE can sidegrades to eachother. https://stats.stackexchange.com/questions/568238/disadvantages-of-using-a-regression-loss-function-in-multi-class-classification . But we are not doing classification, we are doing regression as our prediction is 1-5. For regression, MSE is better suited, but cross-entropy is faster when predictions vary significantly. https://rohanvarma.me/Loss-Functions/ . What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model? | . -pass- . What is the use of bias in a dot product model? | . To encode extra information about the problem, in our case, the fact that some users rate really highly or lowly, and some movies are generally really well rated or badly rated. . What is another name for weight decay? | . L2 regularization . Write the equation for weight decay (without peeking!). | . Add a wd*(w^2) to the loss function, where wd is weight decay, a parameter just to control how much we dislike large weights. . Write the equation for the gradient of weight decay. Why does it help reduce weights? | . add a wd*(w^2) to the loss function, where wd is weight decay. It penalises large weights, because as we try and lower loss, the weight decay term will be large if the weight is too high. . Why does reducing weights lead to better generalization? | . Because larger weights are associated with a more complex total loss function. For instance, large weights results in a very sharp curve with many edges going to specific data points. Having lower weights corresponds to having a more gradual curve that is less overfitted to specific data points. . What does argsort do in PyTorch? | . It can give us the elements with the largest or smallest values of a column we want. In our example, for movie_bias, we can easily see the movies with the lowest biases after training. . Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not? | . No. Specifically looking into the sorted movie biases tells us specific information we can interpret. In our case, the movies with the lowest biases are the movies that people tend not to like even if it&#39;s something that they&#39;d normally enjoy (something that matches their preferences). This is because these movies&#39; latent factors (genre, etc) match the users&#39; latent factors (preferences) but still have very little movie bias. A higher movie bias tells us that users regardless of movie and user latent factors rate that movie highly, so a small movie bias tells us that users rate that movie badly, regardless of how well the movie and user latent factors match. . How do you print the names and details of the layers in a model? | . learn.model . Extra: What is PCA? | . Principal component analysis is a technique to interpret embedding matrices. For humans its hard to understand latent factors, so this helps. We can plot a graph of the strongest PCA components, which are like latent variables, for the movies. And then looking at it, as humans we can try and infer what these PCA components mean, and why they are so valuable for predictions. It&#39;s amazing, since it can discover things about the data with no help. . What is the &quot;bootstrapping problem&quot; in collaborative filtering? | . The extreme case, when we have no users and so no data, what do we recommend to the first user? Or even if we have many users, how do we recommend a new product to them? . How could you deal with the bootstrapping problem for new users? For new movies? | . We can ask new users about their preferences and create a separate model that tries to predict their embedding vector based on this. . How can feedback loops impact collaborative filtering systems? | . Say a small number of dedicated users really like anime. They only watch it rather than other genres, and always rate it. This tunes the recommendation system to recommend more anime, which in turn makes other existing users watch anime, makes users who don&#39;t like anime leave, and attracts new users who like anime. This causes a feedback loop and can make the original purpose of the website change. . When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users? | . In our previous non deep learning model, we took the dot product to use embeddings. But in DL we don&#39;t use the dot product, we use a different way. We take the results of the previous embedding lookup and concatenate its activations. . Why is there an nn.Sequential in the CollabNN model? | . In order to create our neural network layers in the order we want. . What kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model? | . A deep learning model. Our collaborative filtering model dot product approach is hardcoded to only be able to use userID, movieID, ratings, latent variables, and biases. A DL model, or TabularModel, can incorporate more information, however as the next lesson discusses, there are advantages and disadvantages to both. . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson7.html, which includes a lecture, notebooks, and a set of questions from the course book. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson7Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/27/Lesson7Blog.md.html",
            "date": " • Sep 27, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fast.ai Lessons 6 Roundup",
            "content": "Lesson Overview . This lesson focused on the main alternate to deep learning: Random Forests and Gradient Boosting. There is an excellent summary at the end of the book&#39;s chapter giving context into why and when to use deep learning and/or RF/GB. Essentially, the latter is great for tabular data, and the former for more complex messy input like natural language, images etc. How to create a Random Forest from scratch is covered, as well as some links describing Gradient Boosting in more depth. Lastly, the notebooks road to the top part 1 and part 2 are covered giving more insight into how to create good ML models in a competitive setting. . The topics covered, briefly . Random Forests: how to create them from scratch, how they are hard to do incorrectly, how they are hard to overfit, how their hyperparamaters work, what bagging and out-of-bag error is, how they are great for tabular data and are especially good due to their easy understandability and interpretation. | OneR vs TwoR classifier differences and creation. | By comparing OOB error with validation error, you can tell if there are issues with the way your validation data is constructed and/or if there is data leakage. | A list of vision models to use sorted by iteration speed and accuracy. | More insight how to compete in machine learning competitions by creating good validation sets and iterating quickly. | Issues with model iteration speed and CPU/GPU usage. Ideally, pick a model that can utilise both correctly instead of being bottlenecked by either. | . Lecture Notes . Binary splits. They find and use the best value to split a dataset into two parts to predict the dependent variable. | We found Sex to be the best predictor from the columns for surviving the titanic, with the score to be about 0.4, a score being a measure of how good the split is at predicting. | If we predicted all women to survive and all men to die, it would do a decent job. This is a called a OneR classifier. | How to improve this? Use a TwoR classifier, look into the male and female groups separately and use another column to split again! | This gives us a decision tree! A series of binary splits. DecisionTreeClassifier, from sklearn, does it for us. It takes leafnodesize as a variable, meaning for the final groups at the end, at minimum how many datapoints need to be in them. | Gini is like score, it&#39;s another metric for measuring how good the split is. | TwoR might not necessarily be better than OneR at predicting, especially for small datasets. | For tabular data, always use a decision tree. It requires very little preprocessing (don&#39;t have to convert categorical data, for example) and is a good baseline. | Bagging: Build many decision trees using different binary splits and subsections of the training data. Each tree will have an unbiased amount of error: error that is random and uncorrelated. If we average out the errors that are random and uncorrelated we can get very little error left! So build an ensemble of decision trees! This is called a random forest. | For datasets of decent size, Random Forests almost always are better than OneR. | A feature importance plot tells us how important each feature is! It&#39;s amazing for understandability. Maybe for a tabular dataset, we use a random forest feature importance plot first for a baseline and understanding of feature importance, then create a DL model with that comparison and understanding in mind. | After about 30 trees, the improvement for random trees doesn&#39;t improve massively. Roughly &lt; 100 works best. More forests does not cause overfitting, in fact, having too few trees might. This is the same as having a small ensemble of overfitted models could be overfitted, but a large ensemble will not be overfitted. | Because when creating trees, we didn&#39;t train each one on all of the training data, say on a random subsection of 0.75 of it. We can then use the final 0.25 section as validation data for each tree. Then average this error across all our trees. This is called the OOB (out-of-bag) error. | For the random forest, we might be able to get away without using validation data to see if there&#39;s overfitting by using OOB instead. | Random forests are amazing for model interpretation. It gives confidence in predictions, which columns are strong predictors, how related are columns are to eachother etc. | If all the trees are predicting very different things for the same entry, then we are not very confident. | A partial dependance plot uses the forest&#39;s predictions to set all the other data equal and tells us how two variables are related. There is an important distinction. Say we want to know how car price depends on the year of manufacture. We could just plot a graph of car prices and manufacture year. This however would depend on other variables: say another present column, air conditioning (yes/no), greatly improves the price. In contrast, PDP will get the prediction input data and make all the columns&#39; data the same except for year of manufacture. Then it will make predictions for car price using that, and plot it against year of manufacture. Unlike the first plot, it tells us a different measure of how year of manufacture affects price. | Gradient boosting is another meta technique using binary splits like random forests. It&#39;s can more accurate but can overfit so can be done wrong. More info here: https://explained.ai/gradient-boosting/. | . The lecture than goes through a more in depth notebooks than Iterate like a grandmaster called road to the top part 1 and part 2: -Just squish images, it&#39;s better than cropping. Padding would take more precious CPU power for iteration? (See point below). TTA, augmenting input images, can improve accuracy too. . There is another notebook: https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning, showing the best models for vision to use for quick iteration and testing. | The fastai learning rate suggestions are a bit conservative so you can pick higher than them. | Create different notebooks for different approaches. Duplicate and rename them. | Kaggle&#39;s GPUs aren&#39;t amazing, but for Jeremy his home PC ran training so much faster. The problem wasn&#39;t GPU, it was CPU, the Kaggle CPU indicator showed it as full. This is a image input problem. Loading them up requires CPU power. Simply resizing the images by 1/4 quickened up iteration by 4x but without sacrificing accuracy! Perhaps larger image sizes aren&#39;t so important. But later on, with some data augmentation, using larger image sizes helped. | Since the GPU was barely used, you might as well switch to a model that&#39;s more demanding. It probably won&#39;t take much longer. | Keep upto date with new model architecture. ConvNext is a great default speed and performance for now. | . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson6.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/26/Lesson6Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/26/Lesson6Blog.md.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Fast.ai Lessons 5 Roundup",
            "content": "Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson5.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/26/Lesson5Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/26/Lesson5Blog.md.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Masters Project 04/10/2022",
            "content": "Meeting: . During the meeting we discussed: . What I had been learning during the summer, namely the first fast.ai course, the state of exponential growth of machine learning, my website, etc. We are all happy with the progress I have made so far. I will estimate that I spent around 40 hours working this summer, ahead of the 30 hours the department set. | For example, I showed how I can upload machine learning models to HuggingFaceSpaces, which lets me send a link easily to Stuart, Robert, the biology dept etc and let them test and play with what I&#39;m doing. This will be very useful as I physically will not need to take my computer over to get feedback on models. | How the project will be examined: by extended report (like a dissertation), a seminar, a supervisors mark (on your effort and from a viva), and also a provisional formative 10 page report. An external supervisor will be picked at random from 4 academics Stuart and Robert pick to read your report. | Stuart said to take the formative report seriously, because feedback on your writing style etc is really important. Your supervisors can&#39;t give feedback on your real report later on due to department rules. | We will meet with Phil and Steve, from the biology department, next week, meeting at the TLC at 10:55. | Stuart has two audio recording devices that use SD cards to record. He said that he will order SD cards. I could use these to collect sound data if necessary. There is a large amount of data already collected, both by the biology department and available online, but if I wanted data collected in a specific way, then I could do it myself. For example, as we have two devices, I could set both of them up to gather the same audio, and use that to analyse out noise. What I could do, is use the same microphones the biology department did, and use them to remove noise from their whole dataset. | On a ambitious note, the biology department has been using community volunteers to look through photos and label animals in them. In a similar way, if this project is really successful, we could get a grant to lend out a specific recording setup to them and get a lot of specific data. | Robert explained some possible project aims relating to biodiversity. For example, if we could classify birds by audio, we could look for birds that shouldn&#39;t be in the UK or in specific areas. Birds that are migrating in the wrong direction, due to climate change or other environmental issues. This could tell us about biodiversity. If we had data of audio collected over months or years, then about how biodiversity is changing over time. I commented that if the biology department also recorded the GPS location of their recording device, it could be possible to map out how the number of specific birds changes in various areas. | I asked Stuart whether any of his other supervision students are working on a similar problem to me. While he has many students, we are all doing varied projects which is interesting but hinders collaboration. For now, there should be a PhD student working with the biology team with I could work with. | I explained that I want my diss to be written in a way it can be understood by anyone with no prerequisite knowledge. This stems from me expressing that the fundamental mechanisms in which machine learning works is actually very simple, it&#39;s high school level maths and can be explained in one or two pages. This approach is good because it will make examination and marking easier. | I explained that for frequent progress updates, I want to continue writing blogs on this website and link it to the teams channel. Right now one blog a week after the week&#39;s meeting is probably a good pace. | We agreed on my plan to spend half my time working directly on the problem, and the other half going through the second fast.ai course starting Tuesday next week. | . I also attended the intro physics project talk this week: . Level 4 contributes to 44.4% of your final year mark. | My other modules, atomslasersqubits and planets&amp;cosmology, have one self assessed problem a week. | Planets&amp;Cos has 80% exam, and 20% essay. Choose a topic and write 1500-words for deadline 2pm Friday 10th Feb, but from disabilities I may be able to get an extension if needed. | Technical support at physics.level4lab@durham.ac.uk to help with small scale jobs, advice, equipment ordering. Could be useful to setup recording devices for me. | There are weekly Python computing drop in sessions at Ph216 at 12-1pm on Mondays and Thursday with PhD students to help etc. Also there&#39;s an online support website. | Assessment: In order. Seminar 5% between 27th Feb-3rd March. Oral exam 25% between 24th April-2nd May. 40% report, 30 pages, 2pm Weds 19th April deadline. 30% supervisors&#39; mark from supervisors&#39; thoughts about my effort and a viva. | Majority of the diss writing happens over Easter. The deadline is 2pm Weds 19th April, a week before 3rd term. | Upload updates to the project on teams to act as a record. It helps with the supervisors&#39; mark. It helps with external examiners to act as evidence between interaction between student and supervisors. Great for diss writing. | Short 10 page interim report for Christmas assessed by main supervisor. | Advice to write reports in LaTeX, although I think I will write mine in Jupyter notebooks for easy code integration like how the fast.ai textbook is written. | Oral exam/Viva, 25 minutes, with an examiner, moderator, supervisors. No formative practice for this. | Formative 10 minute seminar with supervisors after Christmas. | Can borrow equipment from labs if given permission. Might need to do a risk assessment. | . Next Steps: . This week I want to: . Finish fast.ai part 1. Namely the questions and blog posts for the last 2 lessons. | Go and investigate some of the project&#39;s proposal&#39;s references starting with scikit-maad and a CNN classifier. | Ask for a reimbursement for fast.ai part 2 | . Long term plan: . Spend half my time going at the problem directly. The proposal references. I found that fast.ai have an audio library, fastaudio at https://github.com/fastaudio/fastaudio, and online I found several blog posts about people using it to classify birds. https://www.ecosia.org/search?q=fast%20ai%20fastaudio%20bird%20classification&amp;addon=opensearch. Also it would be good to learn some domain specific knowledge, I.E, knowledge about birds and bird songs, specific knowledge that is helpful to inform ML models. | Spend the other half of my time learning more machine learning knowledge, namely, working through the fast.ai part 2 course starting next week. My reasoning is as follows: the second course covers more advanced topics likely highly important for my Masters. For example, how to implement the most recent ML papers into your work, which requires higher knowledge about how to edit code architecture. And further forward, if I want to add explainable AI modules into my work, this knowledge may prove useful. Lastly, there&#39;s likely many useful topics covered that I don&#39;t get know of. | As a side, this style of plan is inspired by Jeremy Howard, the creator of fast.ai. He said that thinks spending half your time working and half your time learning is a great approach for computer science. The combination of real practice while also learning is good. | .",
            "url": "https://exiomius.github.io/Blogs/mp/2022/09/25/MP2.html",
            "relUrl": "/mp/2022/09/25/MP2.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Masters Project Introductory Post",
            "content": "This is the start of a series of posts about my Masters project with the Physics Department at Durham University. To being with, here is the official project description as presented by the department: . Title: Data science for biodiversity loss . Supervisor: Prof Charles Adams. Second Supervisor: Dr Robert Potvliege. Category: General. Type: Computation/Data Analysis/Experiment. . Biodiversity loss due to human action is increasingly creating an existential threat to all live on Earth. In order to take appropriate action we need better data. However biodiversity data is both more diverse and more difficult to accumulate than say climate data. In this project, we shall look at data analysis on bird song. Although, under ideal conditions it is possible to identify different species [1-3]. In a noisy environment, which is more typical, this becomes more challenging. One approach that we shall pursue is to construct time frequency pattern and then use pattern recognition technique to identify particular events [4]. . [1] scikit‐maad An open‐source and modular toolbox for quantitative soundscape analysis in Python, Ulloa et al, Meth. in Ecology and Evolution, 12, 2334 2021 . [2] Multifractal analysis of birdsong and its correlation structure, R Bishal, GB Mindlin, and N Gupte Phys. Rev. E 105, 014118 2022 . [3] Large-scale analysis of frequency modulation in birdsong data bases, D Stowell, MD Plumbley, Methods Ecol Evol, 5: 901 (2014) . [4] New aspects in birdsong recognition utilizing the gabor transform, S HEUER , P TAFO, H HOLZMANN, S DAHLKE, Proc. of the 23rd Int. Congress on Acousitics, Aachen, September 9-13, 2019. . Aim . In essence, the (first) aim of this project is to pick apart bird song from a noisy sound recording and identify birds from it. . In the future other aims may emerge, such as trying to understand the meaning behind birdsong rather than classify their singers. Perhaps even trying to generate birdsong may be an interesting idea to yield some insights. It would be fascinating seeing if/how birds would respond to generated birdsong. . Approaches . The four above referenced papers are probably the best place to start when looking for initial approaches. While I am comfortable with both Physics and Programming, my preference in comfortability and in interest does lean towards the latter, especially when Machine Learning is involved. Having said that, the first, third and fourth reference all are ML based to a degree. . I don&#39;t yet have the prerequisite knowledge to understand at a glance the abstracts of the second and third references. . The first however is an open-source Python module called scikit-maad, which is instantly recognisable by my familiarity with other popular scikit modules. Furthermore, I feel warmly welcomed by the described online documentation and practical examples around it. Lastly, the module highlights its ability to easily integrate Machine Learning Python packages. . The fourth reference details that current approaches convert audio recordings into spectrograms using the Gabor transform, then enter them as images to a CNN for classification. This is a really intuitive ML approach, and one I might try and implement. The paper then details that most approaches focus on finding the best CNN hyperparameters for accuracy, so in contrast the authors attempt to evaluate the parameters for the Gabor transform itself. . All in all, I think my first priority is to investigate and implement the first and fourth references. It&#39;s not ideal not being able to easily understand the other papers, but my reasoning is that having gone through the more understandable references first would yield prerequisite knowledge to go back and understand the others. . Coincidentally, I was talking briefly with my older brother about the project and he commented that the problem is awfully similar to other audio separation problems. Separating out bird song from a noisy forest environment and then identifying them, is similar to separating out instrument sounds from a regular song and identifying them. We both actually have a mutual friend who did his Masters project in the latter. Spotify also appears to have its own development going on for this problem. Looking through how similar these two problems are might prove very useful. . I will meet with my supervisors next week and discuss our next steps in more detail. The fast.ai part 2 course is releasing soon and I would like to progress through it both for the sake of the project and my own interests. .",
            "url": "https://exiomius.github.io/Blogs/mp/2022/09/25/MP1.html",
            "relUrl": "/mp/2022/09/25/MP1.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Fast.ai Lessons 4 Roundup",
            "content": "Lesson Overview . This lesson was an introduction to natural language processing (NLP). NLP includes tasks such as text classification (e.g. positive or negative sentiment) and text generation (creating new text from a prompt). It appears to be an immensely growing field and as a result certain modules are struggling to keep up to date with the state-of-the-art. A new technique called transformers has recently become popular, and FastAI&#39;s library for example has not been updated with its functionality yet, despite the authors working on it. As a result, this lesson&#39;s lecture and notebooks covers HuggingFace&#39;s transformer module, while the textbook still uses the older FastAI methods. . This dilemma reflects a common issue with growing fields. As the state-of-the-art changes so quickly, one must take care to be flexible and be comfortable with learning many modules and ways of doing things. It doesn&#39;t seem to bad however, because the fundamentals behind the techniques are still not often changed, so learning it once properly will make learning newer techniques quick. Furthermore, updated modules often have really good documentation, tutorials and discussions available. HuggingFace seems to have some brilliant transformer documentation across their webpages and GitHub. Lastly, this &#39;dilemma&#39; actually might be pretty entertaining. Seeing your field grow with changes and improvements and having to learn about them may keep working pretty interesting. In a way it is like also having to be wear the hat of an academic as a part of your usual job role. The variation could be fun. . Notebooks covered . This lesson covered two notebooks,Getting started with NLP for absolute beginners and Iterate like a grandmaster! . &quot;Getting started with NLP for absolute beginners&quot; covers: . How to use Kaggle. Including how to use Kaggle datasets on your own PC and how to submit entries to their competitions. | How to use Pandas and Transformers DataFrames to view data, and get it into the correct format for NLP model training. | How to use Transformers to train and classify. | The difference between training, validation, and testing data. An emphasis has been put on creating good validation data with a quote by Dr Rachel Thomas stating that often a poorly chosen validation dataset results in a disconnect between development and deployment performance. Her article describes this in more detail. | Training data is to train. Validation data is get an idea of generalizable performance, but often it is limited in doing so. This is often either because it hasn&#39;t been chosen prudently enough or because one has accidentally overfitted to it. Imagine a model to predict the price of a stock. Randomly selecting points to be validation data is poor because that is not how the model will be used in practice and is a much easier task. Selecting some amount of further price movement to be validation data makes sense, but your model may overfit to the specific movement pattern of that timeframe. | The Pearson Correlation Coefficient, r, as a metric is discussed. Emphasis is put on firstly trying metrics on datasets to understand them, rather than delving immediately into the maths. Doing so for example yielding the fact that r is really sensitive to outliers. | On outliers. Outliers are important parts of the data and mustn&#39;t be removed without reason. In our housing income and average number of rooms correlation example, there are outliers that could lead to some insights about the data. Perhaps the outliers are from a specific type of house or a specific geographical area. In this case, it may make more sense to use separate models to train on and predict separate clusters of data. | On hyperparameters. Learning rate is the most important in this case. The idea is to find the largest value that doesn&#39;t result in failed training. FastAI provides a learning rate finder to help you, but Transformers does not. | . &quot;Iterate like a grandmaster!&quot; covers: . How a grandmaster Kaggle competitor works. He focused on creating an effective validation set and iterating rapidly to find changes which improve validation set results. These skills carry over to real projects. | For the patent classification, the input is anchor and label is target. In the test data, there are anchors not present in the training data. Thus we should make sure there are anchors in the validation data that are not in the training data. | Pick a learning rate and batch size that fits your GPU. This means picking them so that we can iterate rapidly to test things out. | Pick a reasonable weight decay. | Pick a small number of epochs, like 3, to test with. This is because in practice much of the performance will be made in those. Thus there is no need to run many epochs every time you try a change. If there is not improvement within a few epochs, then your change is likely not very significant. Later on, when you want to more thoroughly evaluate a change, you can use more epochs and cross-validation. | Pick a class to setup your arguments for your trainer. Transformers by default uses one, but FastAI has others. | You need a stable validation accuracy from your epochs to know whether your future changes is making improvements. To know whether your predictions are stable, run the model from scratch a few times, say 3, and check how much it varies. | To make changes easier, create a function to setup tokenization and a function to setup model creation. Then you can pass parameters quickly to create new models. | In this case, previously we tokenised with a special token sep to indicate seperate entities in our input. Simply changing sep to s resulting in a big performance increase. | Instead of using the same special token to indicate separate entities in our input, using different special tokens for each entity could better inform the model that each entity is different. | Simply changing all text to lowercase can often help a little too. | There&#39;s so many things you could try. In this notebook, most of the iteration was done by changing tokenisation, but also playing around with the other parameters might yield better results. However, instead of trying to optimise the factors already present, there are other ideas you can try. Firstly fine tuning your general language model using just patent data. Or using a model pretrained on legal vocabulary instead of general vocabulary. Using a different type of model, not in terms of architecture but a model created for a different task. One of our columns is &#39;context&#39;, which is a code e.g. B7 referring to a patent context. Instead of using the code, we could replace it with a description found online. There&#39;s so many things you can try especially if you think a little creatively. | Remember for the final submission, to train on your validation data as well. | . A subset of this chapter&#39;s questions and my shortened answers are as follows: . What is self-supervised learning? When you don&#39;t use labeled data. You make the model divide the data into input and label itself. Self-supervised learning is often used to train models to be used as a transfer model for a different task such as classification. | What is a language model? A model trained to predict the next word of an input text. | Why do we fine-tune language models? Because the pretrained model wouldn&#39;t have had specifically been calibrated for your specific task. | What is Universal Language Model Fine-Tuning? It is an approach to NLP including transfer models. In order: Find/train a general language model, transfer/fine tune it to make a task specific language model, transfer/fine tune it to make a final model for the task at hand. | How does one need to prepare their data for a language model? Tokenisation, Numericalization and DataLoading must be done. | What is Tokenisation? Tokenisation: Convert the text into a list of words/characters/substrings. The way you do this has to the same throughout all the models you use. If you grab a model online, you have to tokenise your models in the same way. | What is Numericalization? Numericalization: First make a vocab list/dictionary of all the words used in the dataset. They will all be given unique numbers to identify them. Then convert the tokenised text into a list of these numbers. | What is DataLoading? In FastAI LMDataLoader automatically sets the last token in a sequence as a label, as well as other important data preparation processes. | Many details about tokenisation are discussed including special characters, rules, and repeated characters. | Why would a model tokenise some words as &#39;unknown&#39;? Because there is a limit to the number of words/tokens the vocab list/embedding matrix contains. | Why is padding needed for text classification but not language modeling? Because PyTorch&#39;s DataLoader(s) uses tensors that can only store elements of the same type and size. In text classification, the text/documents in tensors are of varying size. In language modeling, they are not. | What is an embedding matrix? I need to spend more time on this because it is a little complicated. It contains a list of vector representations of all the tokens present in the vocab list. | What is perplexity? A performance metric used for judge NLP models. | What is gradual unfreezing? It&#39;s a way of training a model. It is unfreezing one layer at a time. Unfreeze a layer, then training that layer’s parameters, then unfreezing another layer and then train, etc until all layers are unfrozen. | Why is text generation always likely to be ahead of automatic identification of machine-generated texts? Because classifier identification models can be used to create better generation models. And in a way by definition, a better classifier model can only be trained on a better generation model after it has already been released online. | . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson4.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my github repository at https://github.com/exiomius/PDL-Lesson-4 .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/21/Lesson4Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/21/Lesson4Blog.md.html",
            "date": " • Sep 21, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Fast.ai Lessons 3 Roundup",
            "content": "Lesson Overview . In this lesson the mechanisms behind deep learning are explored. Unlike the previous lesson, which was focused on applications and many pieces of new software, this lesson was focused on reading through a textbook chapter and understanding how exactly deep learning operates. . I suppose this knowledge will be required in order to understand how best to create good deep learning models. . The topics covered, briefly . How are images represented in a computer. | How are files in datasets structured. | Python functionality: what is list comprehension, and fact that NumPy/PyTorch are much faster than pure python. | Tensors: a list like structure used in deep learning. Ranks indicating dimensions and shapes indicating the length of each axis. | Loss functions: RMSE and L1 norm. Two loss functions, similar but RMSE penalises larger errors more. | SGD: stochastic gradient descent. A way to make a model learn by updating its weights automatically. | The difference between loss and metrics: Loss is for updating weights, metrics are for human evaluation. | Mini-batches. Each epoch, instead of using one piece of training data to update weights (pure SGD), or using all the training data simultaneously to update the weights (GD), splitting the data into random batches (subsets) of the training data to update the weights. | The sigmoid function: a smooth curve between 0 and 1 used in deep learning as a loss function. It is used because it has a meaningful derivative so allows good weight updates. | DataLoader class: a function to take a dataset and split it into random mini-batches. DataLoaders contains a training a training and validation DataLoader. | ReLU: a rectified linear unit. It&#39;s just a linear function, a line, with negative values = 0. | Activation functions. Also known as nonlinear functions, these are placed between layers of linear functions in neural networks so that the linear functions do not combine into another linear function. | The Universal Approximation Theorem: that two linear layers with an activation function inbetween can approximate any function given the correct weights and enough nodes. In practice however we use more than two linear layers because it works better. | . Much more content was covered in much more detail, including how to program a deep learning model from scratch in Python. . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson3.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository at https://github.com/exiomius/PDL-Lesson-3. .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/19/Lesson3Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/19/Lesson3Blog.md.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Fast.ai Lessons 1&2 Roundup",
            "content": "Lesson Overview . The first lesson did well to summarise an introduction to model creation. While it was content I have already covered throughout previous courses, a quick refresher is good. The teaching philosophy behind the course, to learn by doing in conjunction to theory, is excellent in keeping things engaging and enjoyable. The second lesson covered many new software tools and applications very concisely. Although, I had to spend a lot of time troubleshooting various issues with getting things to work on my PC. . The topics covered, briefly . How to create and train a deep learning model in FastAI | How to better filter and amend training data to get better model performance | How to export a ML model as a file | How to load ML models and allow websites to use them | How to save coding repositories online | How to host and create blog posts that include code easily | . This included Kaggle/Google Collab for creating, training and exporting models. Anaconda for Jupyter Notebooks. Visual Studio Code, a programming IDE, to interact with: HuggingFaceSpaces, a website to host models, and GitHub, a website to store one&#39;s code. It took much time and troubleshooting to understand how to install and use these tools, but the result is the understanding of how to quickly produce deep learning models and make them accessible on the internet, as well as produce blogs containing code, with good practices and expandability behind functionality. . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson2.html, which includes a lecture, a notebook, and a set of questions from the course book. My answers can be found on my GitHub repository below. . My machine learning model &quot;NovaOrToast&quot; attempts to classify whether a given cat photo is my older sister&#39;s cat Toast, or my younger sister&#39;s cat Nova. It can be found at https://huggingface.co/spaces/exiomius/NovaOrToast, where it is possible to upload images and get them classified immediately. . My repository for this can be found at https://github.com/exiomius/NovaOrToast The code for the model creation and training can be found at https://www.kaggle.com/code/adnanjinnah/nova-or-toast-model-creator/edit. One must use an online GPU service like Kaggle or Google Collab to train models due to their computational intensity. It should be possible to link said services with GitHub, which would be best to keep a centralised codebase. . The pipeline to upload a machine learning model online . Firstly create, train and export the model using a online IDE such as Google Collab or Kaggle. This is because the training will take up much GPU computation, so it is better to avoid doing it on one&#39;s own machine. | Create a Jupyter Notebook and import the fully trained model. Use a Python module called Gradio to create a local webpage of your model to test if it works on your local web adress. | Convert your Jupyter Notebook into a .py file. | Using HuggingFaceSpaces, create a space for your new model. | Using Visual Studio Code, clone the repository of your new space, then add in your Jupyter Notebook&#39;s, .py file after conversion. | Create a &#39;requirements&#39;.txt including the modules used so that HuggingFaceSpaces can install them as required. | Using Visual Studio Code, use Git to push your update to HuggingFaceSpaces. | After a few minutes, your model will be avaliable on the HuggingFaceSpaces space you created. The progress behind this will be available to see on your GitHub repository under the &#39;actions&#39; tab. | Not included are many many details of how to specifically do these instructions, as there are too many to throughly convey, especially as much time was spend troubleshooting for my specific machine and operating system (Windows). . Things to improve: . A better way to convert Jupyter Notebooks to .py files. It seems to work only with a settings.ini file, but I was not able to create one that works myself, so had to copy one on the course director&#39;s GitHub, which works but I cannot figure out how to place the resulting .py file in the correct folder. | How to upload Kaggle code to GitHub seamlessly. | A way to automatically update Anaconda&#39;s modules. The default Anaconda update prompts update the app but not always the modules I require. For instance, it took me a while to figure out the Python Pillow module wasn&#39;t updated on Anaconda despite it being a fresh installation. | .",
            "url": "https://exiomius.github.io/Blogs/fast.ai1/2022/09/15/Lesson1&2Blog.md.html",
            "relUrl": "/fast.ai1/2022/09/15/Lesson1&2Blog.md.html",
            "date": " • Sep 15, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://exiomius.github.io/Blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://exiomius.github.io/Blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}