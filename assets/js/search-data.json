{
  
    
        "post0": {
            "title": "Fast.ai Lessons 8 Roundup",
            "content": "Questions . Links . EX . The course page for this sessions is https://course.fast.ai/Lessons/lesson8.html, which includes a lecture, notebooks, and a set of questions from the course book. .",
            "url": "https://exiomius.github.io/Blogs/2022/09/27/Lesson8Blog.md.html",
            "relUrl": "/2022/09/27/Lesson8Blog.md.html",
            "date": " • Sep 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fast.ai Lessons 7 Roundup",
            "content": "Questions . The questions from now on will be a part of the blog post rather than uploaded to the lesson&#39;s GitHub repo. . -What problem does collaborative filtering solve? -How does it solve it? -Why might a collaborative filtering predictive model fail to be a very useful recommendation system? -What does a crosstab representation of collaborative filtering data look like? -Write the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!). -What is a latent factor? Why is it &quot;latent&quot;? -What is a dot product? Calculate a dot product manually using pure Python with lists. -What does pandas.DataFrame.merge do? -What is an embedding matrix? -What is the relationship between an embedding and a matrix of one-hot-encoded vectors? -Why do we need Embedding if we could use one-hot-encoded vectors for the same thing? -What does an embedding contain before we start training (assuming we&#39;re not using a pretained model)? Create a class (without peeking, if possible!) and use it. -What does x[:,0] return? -Rewrite the DotProduct class (without peeking, if possible!) and train a model with it. -What is a good loss function to use for MovieLens? Why? -What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model? -What is the use of bias in a dot product model? -What is another name for weight decay? -Write the equation for weight decay (without peeking!). -Write the equation for the gradient of weight decay. Why does it help reduce weights? -Why does reducing weights lead to better generalization? -What does argsort do in PyTorch? Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not? -How do you print the names and details of the layers in a model? -What is the &quot;bootstrapping problem&quot; in collaborative filtering? -How could you deal with the bootstrapping problem for new users? For new movies? -How can feedback loops impact collaborative filtering systems? -When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users? -Why is there an nn.Sequential in the CollabNN model? -What kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model? . Links . EX . The course page for this sessions is https://course.fast.ai/Lessons/lesson7.html, which includes a lecture, notebooks, and a set of questions from the course book. .",
            "url": "https://exiomius.github.io/Blogs/2022/09/27/Lesson7Blog.md.html",
            "relUrl": "/2022/09/27/Lesson7Blog.md.html",
            "date": " • Sep 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fast.ai Lessons 6 Roundup",
            "content": "Lesson Overview . This lesson focused on the main alternate to deep learning: Random Forests and Gradient Boosting. There is an excellent summary at the end of the book&#39;s chapter giving context into why and when to use deep learning and/or RF/GB. Essentially, the latter is great for tabular data, and the former for more complex messy input like natural language, images etc. How to create a Random Forest from scratch is covered, as well as some links describing Gradient Boosting in more depth. Lastly, the notebooks road to the top part 1 and part 2 are covered giving more insight into how to create good ML models in a competitive setting. . The topics covered, briefly . Random Forests: how to create them from scratch, how they are hard to do incorrectly, how they are hard to overfit, how their hyperparamaters work, what bagging and out-of-bag error is, how they are great for tabular data and are especially good due to their easy understandability and interpretation. | OneR vs TwoR classifier differences and creation. | By comparing OOB error with validation error, you can tell if there are issues with the way your validation data is constructed and/or if there is data leakage. | A list of vision models to use sorted by iteration speed and accuracy. | More insight how to compete in machine learning competitions by creating good validation sets and iterating quickly. | Issues with model iteration speed and CPU/GPU usage. Ideally, pick a model that can utilise both correctly instead of being bottlenecked by either. | . Lecture Notes . Binary splits. They find and use the best value to split a dataset into two parts to predict the dependent variable. | We found Sex to be the best predictor from the columns for surviving the titanic, with the score to be about 0.4, a score being a measure of how good the split is at predicting. | If we predicted all women to survive and all men to die, it would do a decent job. This is a called a OneR classifier. | How to improve this? Use a TwoR classifier, look into the male and female groups separately and use another column to split again! | This gives us a decision tree! A series of binary splits. DecisionTreeClassifier, from sklearn, does it for us. It takes leafnodesize as a variable, meaning for the final groups at the end, at minimum how many datapoints need to be in them. | Gini is like score, it&#39;s another metric for measuring how good the split is. | TwoR might not necessarily be better than OneR at predicting, especially for small datasets. | For tabular data, always use a decision tree. It requires very little preprocessing (don&#39;t have to convert categorical data, for example) and is a good baseline. | Bagging: Build many decision trees using different binary splits and subsections of the training data. Each tree will have an unbiased amount of error: error that is random and uncorrelated. If we average out the errors that are random and uncorrelated we can get very little error left! So build an ensemble of decision trees! This is called a random forest. | For datasets of decent size, Random Forests almost always are better than OneR. | A feature importance plot tells us how important each feature is! It&#39;s amazing for understandability. Maybe for a tabular dataset, we use a random forest feature importance plot first for a baseline and understanding of feature importance, then create a DL model with that comparison and understanding in mind. | After about 30 trees, the improvement for random trees doesn&#39;t improve massively. Roughly &lt; 100 works best. More forests does not cause overfitting, in fact, having too few trees might. This is the same as having a small ensemble of overfitted models could be overfitted, but a large ensemble will not be overfitted. | Because when creating trees, we didn&#39;t train each one on all of the training data, say on a random subsection of 0.75 of it. We can then use the final 0.25 section as validation data for each tree. Then average this error across all our trees. This is called the OOB (out-of-bag) error. | For the random forest, we might be able to get away without using validation data to see if there&#39;s overfitting by using OOB instead. | Random forests are amazing for model interpretation. It gives confidence in predictions, which columns are strong predictors, how related are columns are to eachother etc. | If all the trees are predicting very different things for the same entry, then we are not very confident. | A partial dependance plot uses the forest&#39;s predictions to set all the other data equal and tells us how two variables are related. There is an important distinction. Say we want to know how car price depends on the year of manufacture. We could just plot a graph of car prices and manufacture year. This however would depend on other variables: say another present column, air conditioning (yes/no), greatly improves the price. In contrast, PDP will get the prediction input data and make all the columns&#39; data the same except for year of manufacture. Then it will make predictions for car price using that, and plot it against year of manufacture. Unlike the first plot, it tells us a different measure of how year of manufacture affects price. | Gradient boosting is another meta technique using binary splits like random forests. It&#39;s can more accurate but can overfit so can be done wrong. More info here: https://explained.ai/gradient-boosting/. | . The lecture than goes through a more in depth notebooks than Iterate like a grandmaster called road to the top part 1 and part 2: -Just squish images, it&#39;s better than cropping. Padding would take more precious CPU power for iteration? (See point below). TTA, augmenting input images, can improve accuracy too. . There is another notebook: https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning, showing the best models for vision to use for quick iteration and testing. | The fastai learning rate suggestions are a bit conservative so you can pick higher than them. | Create different notebooks for different approaches. Duplicate and rename them. | Kaggle&#39;s GPUs aren&#39;t amazing, but for Jeremy his home PC ran training so much faster. The problem wasn&#39;t GPU, it was CPU, the Kaggle CPU indicator showed it as full. This is a image input problem. Loading them up requires CPU power. Simply resizing the images by 1/4 quickened up iteration by 4x but without sacrificing accuracy! Perhaps larger image sizes aren&#39;t so important. But later on, with some data augmentation, using larger image sizes helped. | Since the GPU was barely used, you might as well switch to a model that&#39;s more demanding. It probably won&#39;t take much longer. | Keep upto date with new model architecture. ConvNext is a great default speed and performance for now. | . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson6.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6. .",
            "url": "https://exiomius.github.io/Blogs/2022/09/26/Lesson6Blog.md.html",
            "relUrl": "/2022/09/26/Lesson6Blog.md.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fast.ai Lessons 5 Roundup",
            "content": "Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson5.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository, https://github.com/exiomius/PDL-Lesson-5-6. .",
            "url": "https://exiomius.github.io/Blogs/2022/09/26/Lesson5Blog.md.html",
            "relUrl": "/2022/09/26/Lesson5Blog.md.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Title",
            "content": "At the end of the last post, I wrote that: . My current plan is to: Finish Fast.ai part 1. I will prioritise going through scikit-maad. I will however, still register and purchase fast.ai part 2. If time allows (within my free time and during holidays), I will go through the course. . However, as I continued through fast.ai part 1 I discovered their audio library, fastaudio. https://github.com/fastaudio/fastaudio, and several blog posts detailing using it to classify birds: https://www.ecosia.org/search?q=fast%20ai%20fastaudio%20bird%20classification&amp;addon=opensearch .",
            "url": "https://exiomius.github.io/Blogs/2022/09/25/MP2.md.html",
            "relUrl": "/2022/09/25/MP2.md.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Masters Project First Post",
            "content": "",
            "url": "https://exiomius.github.io/Blogs/2022/09/25/MP1.md.html",
            "relUrl": "/2022/09/25/MP1.md.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "FastAI Lessons 4 Roundup",
            "content": "Lesson Overview . This lesson was an introduction to natural language processing (NLP). NLP includes tasks such as text classification (e.g. positive or negative sentiment) and text generation (creating new text from a prompt). It appears to be an immensely growing field and as a result certain modules are struggling to keep up to date with the state-of-the-art. A new technique called transformers has recently become popular, and FastAI&#39;s library for example has not been updated with its functionality yet, despite the authors working on it. As a result, this lesson&#39;s lecture and notebooks covers HuggingFace&#39;s transformer module, while the textbook still uses the older FastAI methods. . This dilemma reflects a common issue with growing fields. As the state-of-the-art changes so quickly, one must take care to be flexible and be comfortable with learning many modules and ways of doing things. It doesn&#39;t seem to bad however, because the fundamentals behind the techniques are still not often changed, so learning it once properly will make learning newer techniques quick. Furthermore, updated modules often have really good documentation, tutorials and discussions available. HuggingFace seems to have some brilliant transformer documentation across their webpages and GitHub. Lastly, this &#39;dilemma&#39; actually might be pretty entertaining. Seeing your field grow with changes and improvements and having to learn about them may keep working pretty interesting. In a way it is like also having to be wear the hat of an academic as a part of your usual job role. The variation could be fun. . Notebooks covered . This lesson covered two notebooks,Getting started with NLP for absolute beginners and Iterate like a grandmaster! . &quot;Getting started with NLP for absolute beginners&quot; covers: . How to use Kaggle. Including how to use Kaggle datasets on your own PC and how to submit entries to their competitions. | How to use Pandas and Transformers DataFrames to view data, and get it into the correct format for NLP model training. | How to use Transformers to train and classify. | The difference between training, validation, and testing data. An emphasis has been put on creating good validation data with a quote by Dr Rachel Thomas stating that often a poorly chosen validation dataset results in a disconnect between development and deployment performance. Her article describes this in more detail. | Training data is to train. Validation data is get an idea of generalizable performance, but often it is limited in doing so. This is often either because it hasn&#39;t been chosen prudently enough or because one has accidentally overfitted to it. Imagine a model to predict the price of a stock. Randomly selecting points to be validation data is poor because that is not how the model will be used in practice and is a much easier task. Selecting some amount of further price movement to be validation data makes sense, but your model may overfit to the specific movement pattern of that timeframe. | The Pearson Correlation Coefficient, r, as a metric is discussed. Emphasis is put on firstly trying metrics on datasets to understand them, rather than delving immediately into the maths. Doing so for example yielding the fact that r is really sensitive to outliers. | On outliers. Outliers are important parts of the data and mustn&#39;t be removed without reason. In our housing income and average number of rooms correlation example, there are outliers that could lead to some insights about the data. Perhaps the outliers are from a specific type of house or a specific geographical area. In this case, it may make more sense to use separate models to train on and predict separate clusters of data. | On hyperparameters. Learning rate is the most important in this case. The idea is to find the largest value that doesn&#39;t result in failed training. FastAI provides a learning rate finder to help you, but Transformers does not. | . &quot;Iterate like a grandmaster!&quot; covers: . How a grandmaster Kaggle competitor works. He focused on creating an effective validation set and iterating rapidly to find changes which improve validation set results. These skills carry over to real projects. | For the patent classification, the input is anchor and label is target. In the test data, there are anchors not present in the training data. Thus we should make sure there are anchors in the validation data that are not in the training data. | Pick a learning rate and batch size that fits your GPU. This means picking them so that we can iterate rapidly to test things out. | Pick a reasonable weight decay. | Pick a small number of epochs, like 3, to test with. This is because in practice much of the performance will be made in those. Thus there is no need to run many epochs every time you try a change. If there is not improvement within a few epochs, then your change is likely not very significant. Later on, when you want to more thoroughly evaluate a change, you can use more epochs and cross-validation. | Pick a class to setup your arguments for your trainer. Transformers by default uses one, but FastAI has others. | You need a stable validation accuracy from your epochs to know whether your future changes is making improvements. To know whether your predictions are stable, run the model from scratch a few times, say 3, and check how much it varies. | To make changes easier, create a function to setup tokenization and a function to setup model creation. Then you can pass parameters quickly to create new models. | In this case, previously we tokenised with a special token sep to indicate seperate entities in our input. Simply changing sep to s resulting in a big performance increase. | Instead of using the same special token to indicate separate entities in our input, using different special tokens for each entity could better inform the model that each entity is different. | Simply changing all text to lowercase can often help a little too. | There&#39;s so many things you could try. In this notebook, most of the iteration was done by changing tokenisation, but also playing around with the other parameters might yield better results. However, instead of trying to optimise the factors already present, there are other ideas you can try. Firstly fine tuning your general language model using just patent data. Or using a model pretrained on legal vocabulary instead of general vocabulary. Using a different type of model, not in terms of architecture but a model created for a different task. One of our columns is &#39;context&#39;, which is a code e.g. B7 referring to a patent context. Instead of using the code, we could replace it with a description found online. There&#39;s so many things you can try especially if you think a little creatively. | Remember for the final submission, to train on your validation data as well. | . A subset of this chapter&#39;s questions and my shortened answers are as follows: . What is self-supervised learning? When you don&#39;t use labeled data. You make the model divide the data into input and label itself. Self-supervised learning is often used to train models to be used as a transfer model for a different task such as classification. | What is a language model? A model trained to predict the next word of an input text. | Why do we fine-tune language models? Because the pretrained model wouldn&#39;t have had specifically been calibrated for your specific task. | What is Universal Language Model Fine-Tuning? It is an approach to NLP including transfer models. In order: Find/train a general language model, transfer/fine tune it to make a task specific language model, transfer/fine tune it to make a final model for the task at hand. | How does one need to prepare their data for a language model? Tokenisation, Numericalization and DataLoading must be done. | What is Tokenisation? Tokenisation: Convert the text into a list of words/characters/substrings. The way you do this has to the same throughout all the models you use. If you grab a model online, you have to tokenise your models in the same way. | What is Numericalization? Numericalization: First make a vocab list/dictionary of all the words used in the dataset. They will all be given unique numbers to identify them. Then convert the tokenised text into a list of these numbers. | What is DataLoading? In FastAI LMDataLoader automatically sets the last token in a sequence as a label, as well as other important data preparation processes. | Many details about tokenisation are discussed including special characters, rules, and repeated characters. | Why would a model tokenise some words as &#39;unknown&#39;? Because there is a limit to the number of words/tokens the vocab list/embedding matrix contains. | Why is padding needed for text classification but not language modeling? Because PyTorch&#39;s DataLoader(s) uses tensors that can only store elements of the same type and size. In text classification, the text/documents in tensors are of varying size. In language modeling, they are not. | What is an embedding matrix? I need to spend more time on this because it is a little complicated. It contains a list of vector representations of all the tokens present in the vocab list. | What is perplexity? A performance metric used for judge NLP models. | What is gradual unfreezing? It&#39;s a way of training a model. It is unfreezing one layer at a time. Unfreeze a layer, then training that layer’s parameters, then unfreezing another layer and then train, etc until all layers are unfrozen. | Why is text generation always likely to be ahead of automatic identification of machine-generated texts? Because classifier identification models can be used to create better generation models. And in a way by definition, a better classifier model can only be trained on a better generation model after it has already been released online. | . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson4.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my github repository at https://github.com/exiomius/PDL-Lesson-4 .",
            "url": "https://exiomius.github.io/Blogs/2022/09/21/Lesson4Blog.md.html",
            "relUrl": "/2022/09/21/Lesson4Blog.md.html",
            "date": " • Sep 21, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "FastAI Lessons 3 Roundup",
            "content": "Lesson Overview . In this lesson the mechanisms behind deep learning are explored. Unlike the previous lesson, which was focused on applications and many pieces of new software, this lesson was focused on reading through a textbook chapter and understanding how exactly deep learning operates. . I suppose this knowledge will be required in order to understand how best to create good deep learning models. . The topics covered, briefly . How are images represented in a computer. | How are files in datasets structured. | Python functionality: what is list comprehension, and fact that NumPy/PyTorch are much faster than pure python. | Tensors: a list like structure used in deep learning. Ranks indicating dimensions and shapes indicating the length of each axis. | Loss functions: RMSE and L1 norm. Two loss functions, similar but RMSE penalises larger errors more. | SGD: stochastic gradient descent. A way to make a model learn by updating its weights automatically. | The difference between loss and metrics: Loss is for updating weights, metrics are for human evaluation. | Mini-batches. Each epoch, instead of using one piece of training data to update weights (pure SGD), or using all the training data simultaneously to update the weights (GD), splitting the data into random batches (subsets) of the training data to update the weights. | The sigmoid function: a smooth curve between 0 and 1 used in deep learning as a loss function. It is used because it has a meaningful derivative so allows good weight updates. | DataLoader class: a function to take a dataset and split it into random mini-batches. DataLoaders contains a training a training and validation DataLoader. | ReLU: a rectified linear unit. It&#39;s just a linear function, a line, with negative values = 0. | Activation functions. Also known as nonlinear functions, these are placed between layers of linear functions in neural networks so that the linear functions do not combine into another linear function. | The Universal Approximation Theorem: that two linear layers with an activation function inbetween can approximate any function given the correct weights and enough nodes. In practice however we use more than two linear layers because it works better. | . Much more content was covered in much more detail, including how to program a deep learning model from scratch in Python. . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson3.html, which includes a lecture, notebooks, and a set of questions from the course book. My answers can be found on my GitHub repository at https://github.com/exiomius/PDL-Lesson-3. .",
            "url": "https://exiomius.github.io/Blogs/2022/09/19/Lesson3Blog.md.html",
            "relUrl": "/2022/09/19/Lesson3Blog.md.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "FastAI Lessons 1&2 Roundup",
            "content": "Lesson Overview . The first lesson did well to summarise an introduction to model creation. While it was content I have already covered throughout previous courses, a quick refresher is good. The teaching philosophy behind the course, to learn by doing in conjunction to theory, is excellent in keeping things engaging and enjoyable. The second lesson covered many new software tools and applications very concisely. Although, I had to spend a lot of time troubleshooting various issues with getting things to work on my PC. . The topics covered, briefly . How to create and train a deep learning model in FastAI | How to better filter and amend training data to get better model performance | How to export a ML model as a file | How to load ML models and allow websites to use them | How to save coding repositories online | How to host and create blog posts that include code easily | . This included Kaggle/Google Collab for creating, training and exporting models. Anaconda for Jupyter Notebooks. Visual Studio Code, a programming IDE, to interact with: HuggingFaceSpaces, a website to host models, and GitHub, a website to store one&#39;s code. It took much time and troubleshooting to understand how to install and use these tools, but the result is the understanding of how to quickly produce deep learning models and make them accessible on the internet, as well as produce blogs containing code, with good practices and expandability behind functionality. . Links . The course page for this sessions is https://course.fast.ai/Lessons/lesson2.html, which includes a lecture, a notebook, and a set of questions from the course book. My answers can be found on my GitHub repository below. . My machine learning model &quot;NovaOrToast&quot; attempts to classify whether a given cat photo is my older sister&#39;s cat Toast, or my younger sister&#39;s cat Nova. It can be found at https://huggingface.co/spaces/exiomius/NovaOrToast, where it is possible to upload images and get them classified immediately. . My repository for this can be found at https://github.com/exiomius/NovaOrToast The code for the model creation and training can be found at https://www.kaggle.com/code/adnanjinnah/nova-or-toast-model-creator/edit. One must use an online GPU service like Kaggle or Google Collab to train models due to their computational intensity. It should be possible to link said services with GitHub, which would be best to keep a centralised codebase. . The pipeline to upload a machine learning model online . Firstly create, train and export the model using a online IDE such as Google Collab or Kaggle. This is because the training will take up much GPU computation, so it is better to avoid doing it on one&#39;s own machine. | Create a Jupyter Notebook and import the fully trained model. Use a Python module called Gradio to create a local webpage of your model to test if it works on your local web adress. | Convert your Jupyter Notebook into a .py file. | Using HuggingFaceSpaces, create a space for your new model. | Using Visual Studio Code, clone the repository of your new space, then add in your Jupyter Notebook&#39;s, .py file after conversion. | Create a &#39;requirements&#39;.txt including the modules used so that HuggingFaceSpaces can install them as required. | Using Visual Studio Code, use Git to push your update to HuggingFaceSpaces. | After a few minutes, your model will be avaliable on the HuggingFaceSpaces space you created. The progress behind this will be available to see on your GitHub repository under the &#39;actions&#39; tab. | Not included are many many details of how to specifically do these instructions, as there are too many to throughly convey, especially as much time was spend troubleshooting for my specific machine and operating system (Windows). . Things to improve: . A better way to convert Jupyter Notebooks to .py files. It seems to work only with a settings.ini file, but I was not able to create one that works myself, so had to copy one on the course director&#39;s GitHub, which works but I cannot figure out how to place the resulting .py file in the correct folder. | How to upload Kaggle code to GitHub seamlessly. | A way to automatically update Anaconda&#39;s modules. The default Anaconda update prompts update the app but not always the modules I require. For instance, it took me a while to figure out the Python Pillow module wasn&#39;t updated on Anaconda despite it being a fresh installation. | .",
            "url": "https://exiomius.github.io/Blogs/2022/09/15/Lesson1&2Blog.md.html",
            "relUrl": "/2022/09/15/Lesson1&2Blog.md.html",
            "date": " • Sep 15, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Fastpages Tutorial Page",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://exiomius.github.io/Blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://exiomius.github.io/Blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Fast.ai part 1",
          "content": "This page contains blog posts about my experience doing the excellent fast.ai 2022 part 1 course. . Posts .",
          "url": "https://exiomius.github.io/Blogs/Fast1/",
          "relUrl": "/Fast1/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "BirdSongMastersProject",
          "content": "This page contains blog posts about my Masters project at Durham University. . Posts .",
          "url": "https://exiomius.github.io/Blogs/MastersProject/",
          "relUrl": "/MastersProject/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "About Me",
          "content": "",
          "url": "https://exiomius.github.io/Blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://exiomius.github.io/Blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}