{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5948eea9",
   "metadata": {},
   "source": [
    "# Fast.ai Lessons 7 Roundup\n",
    "\n",
    "(This post is in progress.)\n",
    "\n",
    "# Lesson Overview\n",
    "\n",
    "\n",
    "# The topics covered, briefly\n",
    "\n",
    "\n",
    "# Lecture Notes\n",
    "Whats inside a neural net. Tweaks to the basic neural net we constructed previously.\n",
    "Going through road to the top, part 3.\n",
    "-A larger model has more parameters so can find more features, but the problem is that it takes GPU memory that isn't as flexible as CPU memory. \n",
    "-How to use as large a model as you like. Kaggle is 16Gb GPU memory. |You can find how much memory a model uses. Training for longer does not use more memory. \n",
    "-Gradient accumeleration to avoid memory overuse problem. We run smaller batch sizes, but modify them as to act and train as if we were using the same normal batch size for all the training data.\n",
    "-Gradient accumleration results are identicle to using a higher memory GPU for certain models. It is for convText and Transformers (NLP). If a model uses batch normalisation, then it won't exactly, but will still be a good model. \n",
    "-Pick a batch size that fits your GPU memory, and generally higher and a multiple of 8 is better. Generally (not always) if you double batch size, half learning rate.\n",
    "-We use ensembling bagging on good models of different arctextures. \n",
    "-At the start it may feel random as to why certain approaches/models are better, but over time as you develop intuiton, it will be less random and more systematic.\n",
    "-Generally, it makes sense to iterate on small models then switch to large models, but there's a better way of ensuring this performance converts correctly. Covered in the second course.\n",
    "Now using part 4 rttp:\n",
    "\n",
    "-Predicting two things instead of one, two dependant variables instead of one. E.g. from a rice photo, the type of rice (10 types) and the disease it may have (10 types).\n",
    "-This requires an understanding of making custom loss functions and a deeper look into how cross entropy loss works.\n",
    "-Make a learner just for disease, and create a new metric function for it.\n",
    "-Cross entropy loss: It goes into the maths using a seperate excel sheet. E.g, trying to predict if a image is a cat,dog,plane,fish or building. Your model outputs 5 numbers (relating to probabilities of each category). CEL first finds the softmax value for each of them. Then it compares the actual value (1 for the correct category, 0 for not) to the softmax value for each category. It multiplies the log of the probability prediction for the correct category by the actual value. \n",
    "-Further info: https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/\n",
    "-Binary cross entropy is just cross entropy for 1 category: is a cat or not.\n",
    "-The loss functions have two types. The F function type and the nn class type. The latter has more parameters to play with.\n",
    "-Change the last node outputs to be the number of categories predicted instead of the usual 1 for classification.\n",
    "-You encode the loss function for the model to know what/how many categories to predict. You sum loss functions for multiple category types and their sub categories.\n",
    "-This new model, that can predict 20 categories, actually is better than a model that just predicts disease type! This is because the training to do other types of predictions helps. Sometimes this approach is better, sometimes not!\n",
    "\n",
    "Notebook collab filtering deep dive\n",
    "-A key part of recommender systems\n",
    "-Movielens dataset of movie ratings. UserID, MovieID, and rating.\n",
    "-How to predict how a user will rate a unrated movie for them: match up the user's movie preferences with the movie's features to give a number prediction. \n",
    "-But we don't know their preferences and the movie features, these are called latent variables, and we can infer them from the dataset.\n",
    "-Let's assume there are 5 latent factors, say like for a movie, it's genre, length etc, we don't set these, we infer them and then can try and interpret what they are. \n",
    "-On choosing the number of latent factors, its hard. Fast.ai has a function to calculate this based on Jeremy's intuition, but you can play around too.\n",
    "-Use SGD to optimise these latent factors after we set a loss function.\n",
    "-Imagine a matrix of the users and their movie ratings. There are missing values for unrated/unseen movies. Collaborative filtering is matrix completion, trying to fill in these missing values.\n",
    "-What is embedding? Just looking something up in an array. An embedding matrix is the array that is looked up. Matrix multiplication in an embedding matrix is the same as looking up index values in a list say as a function in excel. Think about a dot product with a one-hot encoded vector, it just returns the value you're looking up. \n",
    "-Creating a collaborative filtering model from scratch. \n",
    "-Covering python, PyTorch class definition and features. \n",
    "-We create a DotProduct class to define embeddings and looking up values for UserIDs and MovieIDs.\n",
    "-Some of our predictions can greater than 5, the maximum. Take our predictions and squish them with a sigmoid to fix this.\n",
    "-Some users just relates all movies highly, some users have a range of ratings. Let's incoperate this. Make another inference variable, a movie bias and a user bias, reflecting that for movies, they tend to be especially related well or badly, and that for some users, they can rate all movies generally as good or bad. \n",
    "-With more column features, we could cluster users instead to try and incoperate user and movie types/preferences?\n",
    "-We can use L2 regularisation (weight decay), to avoid overfitting. This adds the sum of the square of the weights to the loss function. This also solves the issue of having useless interfered variables, because they won't contribute.\n",
    "-In fast.ai, usually defaults are good, but for tabular data, it's hard to know good defaults, so it's good to test yourself.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9a391",
   "metadata": {},
   "source": [
    "# Questions\n",
    "The questions from now on will be a part of the blog post rather than uploaded to the lesson's GitHub repo.\n",
    "\n",
    "-What problem does collaborative filtering solve?\n",
    "-How does it solve it?\n",
    "-Why might a collaborative filtering predictive model fail to be a very useful recommendation system?\n",
    "-What does a crosstab representation of collaborative filtering data look like?\n",
    "-Write the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!).\n",
    "-What is a latent factor? Why is it \"latent\"?\n",
    "-What is a dot product? Calculate a dot product manually using pure Python with lists.\n",
    "-What does pandas.DataFrame.merge do?\n",
    "-What is an embedding matrix?\n",
    "-What is the relationship between an embedding and a matrix of one-hot-encoded vectors?\n",
    "-Why do we need Embedding if we could use one-hot-encoded vectors for the same thing?\n",
    "-What does an embedding contain before we start training (assuming we're not using a pretained model)?\n",
    "Create a class (without peeking, if possible!) and use it.\n",
    "-What does x[:,0] return?\n",
    "-Rewrite the DotProduct class (without peeking, if possible!) and train a model with it.\n",
    "-What is a good loss function to use for MovieLens? Why?\n",
    "-What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?\n",
    "-What is the use of bias in a dot product model?\n",
    "-What is another name for weight decay?\n",
    "-Write the equation for weight decay (without peeking!).\n",
    "-Write the equation for the gradient of weight decay. Why does it help reduce weights?\n",
    "-Why does reducing weights lead to better generalization?\n",
    "-What does argsort do in PyTorch?\n",
    "Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?\n",
    "-How do you print the names and details of the layers in a model?\n",
    "-What is the \"bootstrapping problem\" in collaborative filtering?\n",
    "-How could you deal with the bootstrapping problem for new users? For new movies?\n",
    "-How can feedback loops impact collaborative filtering systems?\n",
    "-When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\n",
    "-Why is there an nn.Sequential in the CollabNN model?\n",
    "-What kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33becb",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "[EX](https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1)\n",
    "\n",
    "The course page for this sessions is https://course.fast.ai/Lessons/lesson7.html, which includes a lecture, notebooks, and a set of questions from the course book. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
