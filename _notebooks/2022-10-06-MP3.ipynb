{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312fc0bb",
   "metadata": {},
   "source": [
    "# Masters Project 11/10/2022\n",
    "- categories: [MP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121e514",
   "metadata": {},
   "source": [
    "(This post is still in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed599683",
   "metadata": {},
   "source": [
    "# Meeting:\n",
    "During the meeting we discussed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d7500",
   "metadata": {},
   "source": [
    "# Work done \n",
    "- Finished the fast.ai part 1 course lectures, questions, and blog posts:\n",
    "\n",
    "The last two lessons I completed this week can be found at\n",
    "https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson7Blog.md.html \n",
    "and https://exiomius.github.io/Blogs/fast.ai1/2022/09/27/Lesson8Blog.md.html\n",
    "\n",
    "Although I've technically finished the course, the knowledge hasn't been properly internalised until I spend more time creating models and testing things out. For this reason, the course creator Jeremy says that it's entirely normal to finish the course and spend months afterwards revising and practicing it. For now though, it should be enough to start the second half and continue to learn as I practice.\n",
    "\n",
    "- Did research about modern approaches:\n",
    "\n",
    "There is a small google research group with a webpage at https://bird-mixit.github.io/\n",
    "They have published a paper \"IMPROVING BIRD CLASSIFICATION WITH UNSUPERVISED SOUND SEPARATION\" at https://arxiv.org/pdf/2110.03209.pdf. They have not released their classification model online, but have released their sound-separation model at https://github.com/google-research/sound-separation/tree/master/models/bird_mixit.\n",
    "\n",
    "Furthermore, in their paper they talk about a Kaggle competition, BirdCLEF, at https://www.kaggle.com/competitions/birdclef-2022/overview hosted by the Cornell Lab of Ornithology with a $10,000 prize pool. The competition is recent, ending four months ago. While the google team's results are better, this competition contains the code of the participants, including the winner, as well as discussion threads about them. \n",
    "\n",
    "Reverse engineering the competition code may be invaluable to my Masters. To do so however, knowledge from fast.ai part 2 will likely be required as well as a lot of time and effort. Thankfully, Kaggle allows you to post your models and see how they would compare on the leaderboard, so I can continuously try different approaches and evaluate how good they are.\n",
    "\n",
    "An important point is that the google team nor Kaggle competition use accuracy as their final metric. While accuracy initially may seem intuitive to understand, there are many issues with it. That's why the competition used a metric called F1, which is explained by deepmind at https://deepai.org/machine-learning-glossary-and-terms/f-score, and another post about interpreting it at https://inside.getyourguide.com/blog/2020/9/30/what-makes-a-good-f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31143e",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
