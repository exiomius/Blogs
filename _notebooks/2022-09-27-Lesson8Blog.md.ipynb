{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50af50a2",
   "metadata": {},
   "source": [
    "# Fast.ai Lessons 8 Roundup\n",
    "- categories: [Fast.ai1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea78352",
   "metadata": {},
   "source": [
    "# Lesson Overview\n",
    "(This post is in progress.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c993164",
   "metadata": {},
   "source": [
    "# The topics covered, briefly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5948eea9",
   "metadata": {},
   "source": [
    "# Lecture Notes\n",
    "- Back to collaborative filtering notebook, about embedding matrix. \n",
    "- PyTorch keeps track of neural network parameters/weights for you.\n",
    "- movie_bias is the interference parameter we created in order to add bias for movies, user_bias is the one we created in order to add bias for users.\n",
    "- How does our model work? To predict, it trains as normal on the input ratings and output movie ratings, but then we add meaningful interference parameters, movie_bias and user_bias for each movie and user respectively, to adjust with domain specific knowledge. Some movies just everyone likes, so are biased, some users just like every movie, so are biased.\n",
    "- Visualsing embeddings: Plotting a principle component analysis of PCA component 1 and 2, gives a compressed view of how our latent factors effect eachother. It gives us a graph to interpret how the top latent factors are related to eachother. Domain specific knowledge would tell us what there PCA components/inference factors are, and we can try and understand why they relate the way they do.\n",
    "- Embedding distance: calculate how far apart each embedding vector is from a specific movie, aka how similar each movie is compared to that movie based on the latent factors.\n",
    "- Using deep learning for collaborative filtering instead: We use a neural net to try and enter the missing values in the matrix.\n",
    "- we use fast.ai to try and find the embedding matrix side/number of latent variables. \n",
    "- In practice, a combined approach of a neural net and dot product approach is best for collaborative filtering. \n",
    "- In collab filtering, often a small number of users and movies overwealm the rest. In anime for example, some users watch so much anime compared to the rest and this screws up predictions for normal users. It's a higher level task to try and resolve this.\n",
    "- Embedding in NLP: an embedding matrix is just a matrix of latent variables for every word. And as before, the embedding distance calculates the embedding distance between a word and other words, telling us how similar they are based on latent variables.\n",
    "- For tabular data, using a tabular model/learner for it, creates an embedding matrix, and uses it.\n",
    "- Collab filtering, NLP, tabular data neural nets, all use embedding matrices. \n",
    "- Using embedding, latent variables, may be a substitute to doing a lot of feature engineering. \n",
    "- Create and train a neural net, take its embeddings and use it with other models like random forests, it can give a nice performance boost.\n",
    "- One latent variable found to help predict retail store sales, was distance in real life. Actually the embedding matrix's distances for that latent variable matched the distances in real life! It learnt IRL distance was important for predicting retail sales and reflected that!\n",
    "- Convolutions: A CNN is similar to the neural networks before, but for computer vision they have a particular feature: it can find features of images like horizontal and vertical lines of handwritten numbers.\n",
    "- How? It has a filter matrix, say a 3x3 one, and moves around, dot producting and RELUing each 3x3 subset matrix of the original image, producing a smaller image. This is called a convolution. \n",
    "- Why does this work? Because of the values in the filter. E.g. we used a 3x3 matrix with the top elements = 1, middle elements = 0, bottom = -1. The dot product of this will only give the highest value when there's horizontal image in the top row, anything in the middle, and no image in the bottom, aka whenever there's a horizontal part of an image! In vertical parts, the bottom row will cancel out the top and produce 0s, aka the convolution layer will have nothing.\n",
    "- The 3x3 matrix is called a filter.\n",
    "- We can have a horizontal edges conv layer, and a vertical edges conv layer. \n",
    "- We use another filter, but with two matrices, one for each conv layer, and to combine their features. \n",
    "- The way we find the kernal values is to set them as parameters and use SGD to optimise. It will find the best kernal parameters for optimum digit classification. \n",
    "- Nowadays we do a stride convoultion, e.g. stride 2, by skipping values. This reduces the grid size by 2x2. The grid size is not reduced by the kernal size, it is reduced determined by the stride we set.\n",
    "- When we're done with stride convos, with aboug 7x7 image at the end, we do an average pool, we average the final activations. \n",
    "- Say we're trying to identify a picture of a bear. If the bear is a small part of the picture, max pool is much better than avg pool at this. It checks each end activation for a bear, rather than having one prediction of bear or not bear for all the activations combined. \n",
    "- Depending on how you want your model to work, pick max or avg pool accordingly. Fast.ai does this for you, it actually does an average of max/avg pool that tries both for you.\n",
    "- A convolution, all the kernal multiplies done, mathematically is equivilent to just one big matrix multiplcation.\n",
    "- Dropout: we can have a dropout mask, we multiply it by our filtered image, to delete random bits of it. Higher dropout means the image is harder to see. A human is able to look at a droppedoutted image and still know what the image is. A model should be able to as well, if we dropout, perhaps it is forced to learn more fundamental features, will should help against overfitting a lot.\n",
    "- There are more activation functions/activations than ReLU. This doesn't matter much so we don't care. \n",
    "- What to do before part 2: \n",
    "- Read Meta Learning: How to Learn Deep Learning.\n",
    "- Watch the videos again and code and experiment as you go.\n",
    "- Spend time on the forums.\n",
    "- Get together with others to study together.\n",
    "- Build projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9a391",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33becb",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "[EX](https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1)\n",
    "\n",
    "The course page for this sessions is https://course.fast.ai/Lessons/lesson8.html, which includes a lecture, notebooks, and a set of questions from the course book. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
